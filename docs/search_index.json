[["index.html", "Методы анализа больших данных в исследованиях поведения покупателей Intro", " Методы анализа больших данных в исследованиях поведения покупателей Ph.A.Upravitelev 2023-12-05 Intro "],["c1_intro.html", "О курсе и аналитиках Запись занятия Обо мне Contacts О курсе Виды аналитиков Продуктовая аналитика Стадии развития продукта Бизнес-модели Полезные материалы Домашнее задание", " О курсе и аналитиках Запись занятия Обо мне продуктовый аналитик в Pixonic продуктовый аналитик в GameInsight аналитик в Консультант+ аспирант СПбГУ (когнитивная психология) Contacts @konhis в mar221.slack.com (основное средство коммуникации) upravitelev@gmail.com (дополнительное средство коммуникации) +7-965-425-5919 или @konhis в telegram (для экстренных случаев) О курсе темы лекций Введение в цели и задачи продуктовой аналитики Метрики активности и вовлечения пользователей Основы юнит-экономики и метрики монетизации пользователей Проверка гипотез А/B-тесты Создание и тестирование фич (feature) факультативно если будет достаточно желающих, можно одно-два занятия посвятить основам SQL. Ни в домашках, ни в контрольных знание SQL не потребуется, это именно дополнительные темы. я хочу пригласить несколько аналитиков-практиков, чтобы они рассказали о каких-то частных темах из жизни аналитиков. формы контроля две контрольные работы две домашние работы накопительная оценка по формуле 0.1 * Кр1 + 0.35 * Др1 + 0.2 * Кр2 + 0.35 * Др2, округление арифметическое. Виды аналитиков data scientists Датасаентисты - общее определение нескольких профессий. Основной набор навыков – математика, программирование и знание бизнес-задач. Сочетание этих навыков в разных пропорциях и характеризует разные виды датасаентистов. Аналитики должны хорошо понимать бизнес-задачи проекта и специфику бизнеса, к тому же сейчас профессия аналитика предполагает хорошее знание статистики и хотя бы начальные навыки программирования. web-аналитика Задачи: сбор и анализ данных о посетителях веб-сайтов и их поведении на сайте Инструменты: Google Analytics, Яндекс.Метрика, Google Tag Manager маркетинговая аналитика Задачи: оценка эффективности маркетинга (UA, привлечение пользователей) Инструменты: Amplitude, Appsflyer, Facebook etc Есть маркетинговая аналитика, которая касается исследований рынка и так далее. Там совершенно иные требования к навыкам и задачи. Продуктовая аналитика что такое продукт Все, что может быть предложено на рынке с целью удовлетворения чьих-либо желаний и потребностей. В IT под продуктом обычно понимают приложение или какой-то функционал приложения. Соответственно, продуктовая аналитика — анализ того, как пользователи взаимодействуют с приложением и предложенным функционалом (и как за него платят). Близко к web-аналитике, но отличается более детальными данными про пользователя и его поведение, а не просто статистику страниц и переходов. CX/UX-исследователи Тоже близки к продуктовым аналитикам, но больше ориентированы на опыт пользователя и то, как он взаимодействует с приложением (интерфейс) и как использует приложение для решения своих задач. Основные методы – интервью, опросы, фокус-группы, UX-тесты и т.д. структура команды разработки Продуктовые аналитики тесно взаимодействуют с командами разработки (особенно если это мобильные приложения), в основном с продюсерами и разработчиками (особенно на этапе построения систем аналитики новых продуктов), с отделом маркетинга, существенно реже - с коммьюнити-менеджерами. продакт-менеджер / продюсер проджект-менеджер (PM) разработчики (клиент/сервер) дизайнеры (арт), UI аналитики тестировщики системные администраторы коммьюнити саппорт роли продуктовых аналитиков калькулятор интерфейс к базе данных специалист по дашбордам аналитик фич и апдейтов генератор идей / мастер на все руки Стадии развития продукта этапы жизни технические этапы Концепт Прототип Продукт, готовый к запуску Soft launch Global launch Оперирование Поддержка Чаще всего, конечно, продуктовые аналитики работают с продуктом в стадии оперирования - когда идет эволюционное развитие, постоянный приток новых пользователей и есть активная команда разработки. Основные задачи: анализ фич (функционала), контроль баланса, улучшение UX, поддержка продактов при проектировании новых фич. Также аналитики работают с продуктом, готовым к первому запуску и на этапах soft/global launch. Это периоды построения системы аналитики и тестирование, как пользователи реагируют в целом на продукт и на ключевой функционал продукта. Бизнес-модели компоненты все, что связано с разработкой и производством продукта все, что связано с продажей продукта, от поиска нужных клиентов до распространения продукта все, что связано с тем, как клиент будет платить и как компания будет зарабатывать варианты Производство Дистрибьюция Freemium Подписные модели Агрегаторы и уберизация as a Service … тысячи их Полезные материалы рассказ Алексея Натекина про виды датасаентистов рассказ Валерия Бабушкина про то, почему датасаентист - очень общий термин Неплохая статья одного из аналитиков Яндекса. Его мысль про партизанской продакт-менеджмент наглядно описывает, какая роль аналитика в команде самая эффективная и, в общем-то, интересная. Хороший доклад про роли аналитиков в продуктовых (в первую очередь геймдев) командах. Немного многословно, но основные пункты освещены. пара слов про бизнес-модели (я частично ориентировался на эти материалы): раз, два и три телеграм-канал про стартапы, любопытно посмотреть, какие вообще бывают идеи стартапов и на удовлетворение каких потребностей они ориентированы Домашнее задание Промежуточные задания необязательны и нужны для тех, кто хочет развивать свои навыки в области аналитики или в R/Python/SQL. Те, кто решит выполнять задания и кому нужны будут комментарии — присылайте решения в личку в slack. О занятиях, которые будут оцениваться, я сообщу отдельно и не один раз. Для тех, кто не был на прошлом курсе Проверьте и при необходимости отредактируйте профиль в Slack: необходимо указать имя и фамилию (латиницей, в виде Name Family name), по возможности поставьте свою фотографию на аватарку. Всем: В канале #welcome напишите несколько слов о себе: какой опыт работы с R и вообще языками программирования, есть ли опыт работы аналитиком (и где, если есть). Какие ожидания от курса, какие темы вам интереснее всего. подумайте и напишите, каких специалистов и из каких компаний вы хотели бы послушать (кого стоит попробовать пригласить). Не какие-то конкретные люди, а роли. посмотрите на ваши установленные приложения и подумайте, какие ваши потребности они реализуют попробуйте определить, как организован поток денег от вас к компании в ваших приложениях, за что вы платите и как попробуйте определить самое интересное для себя приложение с точки зрения потребностей и их монетизации Если есть желание развиваться в сфере аналитики и продуктовой аналитики: напишите мне о своем желании в личку в slack и скажите, какие навыки лично вы хотели бы подтянуть во время курса. поищите различные вакансии веб-аналитиков, продуктовых и маркетинговых аналитиков. Посмотрите требуемые основные навыки: что из этого вы уже умеете, чему хотели бы научиться, а чем даже понятия не имеете. Определите зону или траекторию своего развития. Если считаете, что я могу помочь вам с этим – напишите, попробуем. "],["метрики-вовлечения-pt1.html", "Метрики вовлечения pt1 Запись занятия AARRR фреймворк User Aquisition Активность и вовлечение Полезные материалы Домашнее задание", " Метрики вовлечения pt1 Запись занятия AARRR фреймворк User Aquisition Метрики привлечения пользователей в основном используются маркетинговыми аналитиками и специалистами по user aquisition, привлечению пользователей. Продуктовые аналитики в основном работают с метриками стоимости пользователя: CPA (cost per action), CPI (cost per install), хотя иметь представления о прочих метриках тоже надо. Процесс привлечения пользователей рекламодатель (тот, кто хочет привлечь пользователей) аукцион рекламной площадки (рекламная площадка выбирает, кому, когда и по какой цене показыть рекламные материалы) целевые действия (установка, платеж и т.д.) (в зависимости от того, на выполнение какого целевого действия оптимизируется рекламная сеть, в приложение будут приходить разные пользователи - те, кто вероятнее всего установит приложение/сделает платеж / сделает другое целевое действие) управление кампанией - таргетинг, бюджет, креативы (рычагов управления рекламными кампаниями не так уж и много - на кого ориентируем рекламу, какой бюджет в день рекламная сетка может потратить на привлечение пользователей, какие рекламные атериалы показываем) Маркетинговая воронка в мобильных приложениях Когда пользователи видят рекламу, они проваливаются в “воронку” — последовательность шагов, которые приводят пользователя в приложение. Вообще воронки — полезный инструмент для оценки, где и на каком этапе отваливается пользователь. реклама (баннер, playable, прочий креатив): CPM (cost per mille - сколько платим за каждые 1000 показов рекламных материалов), CPC (cost per click - сколько платим рекламной сетке за каждый клик по баннеру), СTR(click through rate – клики / показы) переход в стор установка приложения (CR, conversion rate, регистрации / показы) целевое действие (CPI, CPA) Новый пользователь Основная цель рекламных кампаний – привлечение пользователей в приложение. Одна из базовых метрик этого процесса – количество новых пользователей. Однако есть сложности с самим определением, что такое новый пользователь. В частности, считать инсталлом новое физическое устройство. Или новый аккаунт пользователя. Или пользователя, который сделал покупку / подписку (e-comm, в частности). Например, когда пользователь на новый телефон устанавливает приложение и туда происходит логин с помощью его гугл/эппл аккаунта. В этом смысле устройство новое (а маркетинг закупает девайсы), а пользователь старый. Или когда пользователь пользовался приложением на телефоне. Потом удалил и через пару лет увидел рекламу и поставил заново и создал новый аккаунт. Или просто отдал телефон кому-то. Девайс старый, а пользователь новый. Другая история с новыми пользователями – это механизмы ретаргетинга. Это когда мы стараемся вернуть в приложение пользователей, которые уже были нашими пользователями, но потом отвалились. Например, мы выбираем набор девайсов с каким-то суммарным платежом более X единиц, и просим рекламной сетке именно этим устройствам показать нашу рекламу, с помощью которой мы надеемся вернуть пользователей. Этих пользователей сложно считать новыми, но рекламная кампания на них была и, соответственно, сколько-то мы потратили на возвращение этих пользователей. Активность и вовлечение Другая группа метрик - метрики активности и вовлечения пользователей в продукт. К этим метрикам относят обычно количество заходов пользователя в день (количество сессий), количество уникальных пользователей, заходящих в день в приложение. В некоторых случаях считают более длинные метрики - количество уникальных пользователей, зашедших в приложение в последнюю неделю/месяц. DAU, WAU, MAU Daily Active Users Weekly Active Users Monthly Active Users Основная метрика - DAU, как наиболее гибкая и быстро реагирующая на изменения в продукте. Месячные и недельные метрики считаются в скользящем окне, за последние 30 и 7 дней для каждой даты соответственно. Stickness / Sticky factor Иногда смотрят отношение DAU/MAU и интерпретируют как метрику залипания пользователя в проект, его лояльности. В целом это метрика вполне хорошо заменяется метриками удержания (retention). Retention rate Метрика удержания пользователя (retention) — какая доля пользователей вернулась в приложение. Во многом формула расчета ретеншена зависит от того, что мы считаем точкой отсчета. Когда речь идет о мобильных приложениях развлекательного плана (игры, стриминговые сервисы и проч.), то точкой отсчета обычно считают день инсталла, когда пользователь установил приложения. В некоторых продуктах может быть иначе, например, в e-commerce или в сервисах, предлагающих определенные услуги оффлайн (доставка продуктов), считаются только возвраты тех пользователей, которые сделали уже платеж. В целом, метрика удержания одна из важнейших в аналитике - она позволяет понимать, насколько пользователям интересно приложение (сервис), останутся ли они в нем. Соответственно, это прямо влияет на монетизацию: когда пользователи остаются, они либо больше платят, либо, как минимум, есть шансы их побудить сделать платеж (скидками, новыми фичами и т.д.) Нюансы: install day = day 0: традиционно день инсталла считается нулевым днем. day 1/7/14/28: полезно иметь в виду, что бывают циклы, например, ретеншен в течение недели может варьировать в определенном диапазоне. Соответственно, сравнивать два периода/объекта/тестовых группы хорошо бы по одному и тому же по структуре интервалу. проблема интервала (сутки vs календарная дата): обычно считается ретеншен по календарным дням, то есть, если произошла смена даты, то это уже другой день, даже если пользователь установил приложение в 23.55. Временами встречаются вычисления ретеншена строго по 24 часовым интервалам (вернувшийся в игру через 24 часа). Метрики удержания по этим двум формулам вычисления различаются, всегда надо уточнять, как именно велся расчет. rolling retention: иногда нет возможности логировать каждый заход пользователя в приложение, поэтому используется только дата последнего захода пользователя в приложение - то есть, считается, какая доля пользователей заходила после N дня от инсталла. Иногда retention 1 дня / удержание 1 дня сокращают до ret1 / ret1d, r1 (номер дня может быть любым, не только 1). однородность когорт: когда мы считаем удержание по когорте пользователей (например, пришедшим в сентябре), то мы должны считать ретеншен только того дня, который могли прожить все пользователи. То есть, на момент 3 сентября нельзя считать ретеншен 7 дня для тех, кто пришел в приложение 31 августа - они принципиально не могли прожить 7 дней, максимум - 2 (день инсталла и 1-2 октября, 3 сентября также нельзя считать, так как день еще не закончился). Соответственно, по всей месячной когорте можно считать только ret2, даже для тех, кто пришел в начале сентября и мог провести в приложении больше дней. Иногда это минимальное количество дней, которые могли прожить пользователи всех когорт, называют окном лайфтайма. Churn rate Отвалы (churn, отток) - ситуация, когда пользователь окончательно уходит из приложения. Как правило, это достаточно определить, что пользователь больше не вернется, поэтому операционализируют в духе “отвалившийся пользователь - пользователь, который был неактивен последние N дней”. Также как и ретеншен, операционализация отвала может зависеть от приложения и сервиса. Стоить помнить, что отток не тождественен удержанию с другим знаком, хотя достаточно близок по смыслу. Sessions per day Еще одна метрика вовлеченности пользователя в продукт - сколько раз пользователь открывает приложение в течение дня. В более общем виде - какие-то значимые активные действия в единицу времени. Количество сессий в день можно интерпретировать как степень рутинизированности, включенности в повседневные практики пользователя. Для разных продуктов и сервисов, само собой, будут свои критерии - для игр жанра match3 нормально, если пользователь 4-6 раз в день открывает приложение. А вот для приложения оплаты штрафов или банковских приложений это была бы странная метрика, там вообще могут потребоваться другие способы измерения и вовлечения. Полезные материалы Статья про Rolling retention и Retention rate от Олега Якубенкова. Статья от dev2dev про Retention. Список фреймфорков, которые используют продуктовые менеджеры в своей работе. Оффтоп: cмешной случай, как потеряли информацию о 16 тысячах заболевших граждан. Хороший пример, почему для работы с большими данными (да и просто с данными) Excel не очень полезен. Домашнее задание Домашние занятия для желающих. Если будут вопросы или необходимость получить от меня какие-то комментарии - пишите в личку в slack. Задание можете выполнять на любом доступном вам языке / среде для статистики. level 1 (IATYTD) Прочитайте конспект. Обновите знания по работе с табличками — аггрегации (групировки), слияния, создание и модификация колонок. Ссылка на конспекты прошлого курса: https://hse_mar.gitlab.io/mar221s/ (если у вас ссылка не открывается без VPN, скажите мне об этом, пожалуйста) level 2 (HNTR) Необходимо подсчитать и нарисовать, сколько пользователей в день приходит в приложение, в том числе и с разбивкой по платформам. Датасет: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv level 3 (HMP) Используя датасет по заходам пользователей в приложение (dau.csv), подсчитайте и отобразите на графике, сколько пользователей в день заходит в приложение (DAU). Ссылка на файл. Осторожно, файл около 400мб. level 4 (UV) На основе данных по логинам нарисуйте area plot DAU проекта, в котором цветами выделите группы пользователей по количеству дней с момента инсталла: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла У вас должно получится что-то вроде слоеного пирога, где цветами выделены группы. Подумайте, есть ли необходимость рисовать этот график не в абсолютных числах (количество пользователей), а в долях каждой группы от DAU, в чем могут быть плюсы и минусы такого графика. Возможно, вам потребуется нарисовать графики разных типов, чтобы ответить на этот вопрос. Попробуйте подумать, что говорит подобный график о продукте и его пользователях. Есть ли у него проблемные зоны, над которыми надо поработать или которые могут влиять на стратегию развития и/или оперирования продукта? level 5 (N) Постройте графики DAU, MAU и их отношения для данных за июль. Проинтерпретируйте метрику DAU/MAU, что она говорит о проекте? "],["метрики-вовлечения-pt2.html", "Метрики вовлечения pt2 Запись занятия Расчет retention Домашнее задание", " Метрики вовлечения pt2 Запись занятия level 2 (HNTR) Необходимо подсчитать и нарисовать, сколько пользователей в день приходит в приложение, в том числе и с разбивкой по платформам. Датасет: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Решение: # подключаем пакеты (они до этого должны быть установлены) library(data.table) library(plotly) # если есть ошибка с %&gt;%, то явно подключаем соответствующий пакет library(magrittr) # импортируем данные installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) # считаем количество уникальных пользователей по дням intalls_stat &lt;- installs[, list(n_users = uniqueN(user_pseudo_id)), by = list(dt, media_source)] # сортируем по дате инсталла intalls_stat &lt;- intalls_stat[order(dt)] # рисуем график plot_ly(intalls_stat, x = ~dt, y = ~n_users, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;none&#39;, stackgroup = &#39;one&#39;) %&gt;% layout( title = &#39;Установки приложения по дням&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) level 4 (UV) На основе данных по логинам нарисуйте area plot DAU проекта, в котором цветами выделите группы пользователей по количеству дней с момента инсталла: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла У вас должно получится что-то вроде слоеного пирога, где цветами выделены группы. Подумайте, есть ли необходимость рисовать этот график не в абсолютных числах (количество пользователей), а в долях каждой группы от DAU, в чем могут быть плюсы и минусы такого графика. Возможно, вам потребуется нарисовать графики разных типов, чтобы ответить на этот вопрос. Попробуйте подумать, что говорит подобный график о продукте и его пользователях. Есть ли у него проблемные зоны, над которыми надо поработать или которые могут влиять на стратегию развития и/или оперирования продукта? Решение: # импортируем даатсает dau &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv&#39;) # считаем количество дней от инсталла dau[, lifetime := login_dt - install_dt] # делим на группы dau[, lifetime_group := cut(lifetime, breaks = c(-Inf, -1, 0, 7, 28, Inf), ordered_result = TRUE)] # если хотим перезадать порядок уровней # dau[, lifetime_group_ := factor(lifetime_group, # levels = c(&#39;(-1,0]&#39;, # &#39;(28, Inf]&#39;, &#39;(0,7]&#39;, # &#39;(7,28]&#39;, &#39;(-Inf,-1]&#39;))] # второй стобоб разметить группы # dau[lifetime == 0, lifetime_group_3 := &#39;0. 0 day&#39;] # dau[lifetime &gt;= 1 &amp; lifetime &lt;= 7, lifetime_group_3 := &#39;1. 1-7 days&#39;] # dau[lifetime &gt;= 8 &amp; lifetime &lt;= 28, lifetime_group_3 := &#39;2. 8-28 days&#39;] # dau[lifetime &gt;= 28 &amp; lifetime &lt;= 90, lifetime_group_3 := &#39;3. 28+ days&#39;] # создаем отдельную группу для тех, про кого мы не знаем # dau[is.na(lifetime_group), lifetime_group_3 := &#39;unknown&#39;] # третий метод, с помощью fcase # dau[, lifetime_group := fcase( # lifetime == 0, &#39;0 дней&#39;, # lifetime &gt;= 1 &amp; lifetime &lt;= 7, &#39;1-7 дней&#39; # )] # считаем DAU dau_stat &lt;- dau[, list(n_users = uniqueN(user_pseudo_id)), keyby = list(login_dt, lifetime_group)] dau_stat[, total_users := sum(n_users), by = login_dt] dau_stat[, share := n_users / total_users] # area-plot plot_ly(dau_stat, x = ~login_dt, y = ~n_users, color = ~lifetime_group, type = &#39;scatter&#39;, mode = &#39;none&#39;, stackgroup = &#39;one&#39;) %&gt;% layout( title = &#39;DAU по группам пользователей&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) # график линиями plot_ly(dau_stat, x = ~login_dt, y = ~n_users, color = ~lifetime_group, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;DAU по группам пользователей&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) level 5 (N) Постройте графики DAU, MAU и их отношения для данных за июль. Проинтерпретируйте метрику DAU/MAU, что она говорит о проекте? Решение. Строим график MAU. # берем интересующие нас дни dates &lt;- dau[login_dt &gt;= &#39;2022-07-01&#39;, sort(unique(login_dt))] # проходим циклом lapply mau_stat &lt;- lapply(dates[1:2], function(x) { # берем данные в интервале &quot;наша дата - 30 дней -- наша дата&quot; result &lt;- dau[login_dt &gt;= x - 30 &amp; login_dt &lt;= x] # считаем, сколько пользователей заходило за это время (mau) result &lt;- result[, list(dt = x, dt_lb = x - 30, mau = uniqueN(user_pseudo_id))] result }) # собираем все в табличку mau_stat &lt;- rbindlist(mau_stat) # аналогичное решение, более современное по функциям # + считаем одновременно dau и mau library(purrr) mau_stat &lt;- map_df(dates, function(x) { result &lt;- dau[, list( dt = x, dt_lb = x - 30, metric_dau = uniqueN(user_pseudo_id[login_dt == x]), metric_mau = uniqueN(user_pseudo_id[login_dt &gt;= x - 30 &amp; login_dt &lt;= x]) )] result }) setDT(mau_stat) # считаем stickiness mau_stat[, stickiness := metric_dau / metric_mau] # рисуем DAU и MAU plot_ly(mau_stat, x = ~dt, y = ~metric_mau, type = &#39;scatter&#39;, mode = &#39;lines&#39;, name = &#39;MAU&#39;) %&gt;% add_trace(y = ~metric_dau, name = &#39;DAU&#39;) %&gt;% layout( title = &#39;DAU и MAU&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) # рисуем stickiness plot_ly(mau_stat, x = ~dt, y = ~stickiness, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;DAU / MAU&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Расчет retention Общая логика расчета: - считаем lifetime - считаем количество пользователей на каждый день от инсталла - считаем долю этих пользователей от всего пользователей когорты - ограничиваем на общий доступный лайфтайм - рисуем график - опционально – добавляем группировку # берем только тех, кто пришел в июне retention &lt;- dau[install_dt &gt;= &#39;2022-06-01&#39;] retention &lt;- retention[install_dt &lt; &#39;2022-07-01&#39;] # ограничиваем на минимальное общее количество дней retention &lt;- retention[lifetime &lt;= 30 &amp; lifetime &gt;= 0] # считаем количество вернувшихся retention_stat &lt;- retention[, list(returned = uniqueN(user_pseudo_id)), keyby = list(platform, lifetime)] # считаем,с колько всего было retention_stat[, total_users := returned[lifetime == 0], by = platform] # второй вариант расчета total_users, через merge retention_stat &lt;- merge( retention_stat, retention_stat[lifetime == 0, list(platform, total_users_2 = returned)], by = &#39;platform&#39;, all.x = TRUE ) # считаем retention retention_stat[, ret := returned / total_users] # рисуем график plot_ly(retention_stat, x = ~lifetime, y = ~ret, color = ~platform, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Retention rate&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Домашнее задание level 1 (IATYTD) Внимательно разберите решения заданий (материалы конспекта). level 2 (HNTR) Постройте график ретеншена для когорты пользователей, пришедшей в июне, с разбивкой по источникам привлечения (media_source). Для этого вам потребуются следующие датасеты: Инсталлы: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Логины: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv level 3 (HMP) Постройте линейный график retention 1 day (ret1) для всех дневных когорт. Т.е. по оси OX должна быть дата инсталла, по оси OY – значение ретеншена первого для пользователей, пришедших в этот день. level 4 (UV) Добавьте на этот график группировку по источникам трафика (media_source). level 5 (N) Постройте и сравните графики rolling retention и retention rate (возьмите данные за логины и инсталлы из практикума). Для rolling retention необходимо: посчитать максимальный лайфтайм пользователя gосчитать количество пользователей по лайфтайму cделать обратную кумулятивную сумму cумму поделить на количество установок (для lifetime == 0 значения количества инсталлов и обратная кумсумма должны совпадать) "],["c4_monetization.html", "Метрики монетизации pt.1 Запись занятия Разбор домашнего задания Платежные метрики Полезные материалы Домашнее задание", " Метрики монетизации pt.1 Запись занятия Разбор домашнего задания level 2 (HNTR) Постройте график ретеншена для когорты пользователей, пришедшей в июне, с разбивкой по источникам привлечения (media_source). Для этого вам потребуются следующие датасеты: Инсталлы: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Логины: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv library(data.table) library(plotly) installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) dau &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv&#39;) # присоединяем источники трафика retention &lt;- merge( installs[dt &lt; &#39;2022-07-01&#39;, list(user_pseudo_id, dt, media_source)], dau[, list(user_pseudo_id, login_dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # перекодируем retention[, uniqueN(user_pseudo_id), by = media_source] ## media_source V1 ## 1: applovin_int 36714 ## 2: &lt;NA&gt; 36169 ## 3: other 6869 ## 4: unityads_int 21932 ## 5: googleadwords_int 7767 ## 6: Facebook Ads 1297 ## 7: organic 32 retention[is.na(media_source), media_source := &#39;organic&#39;] retention[media_source == &#39;other&#39;, media_source := &#39;organic&#39;] # вычисляем лайфтайм retention[, lifetime := login_dt - dt] # убираем реинсталлы и ограничиваем на минимальное общее окно retention &lt;- retention[!user_pseudo_id %in% retention[lifetime &lt; 0, unique(user_pseudo_id)]] retention &lt;- retention[lifetime &lt;= 30] # читаем количество вернувшихся пользователей retention_stat &lt;- retention[, list(returned = uniqueN(user_pseudo_id)), keyby = list(media_source, lifetime)] # считаем всего пользователей retention_stat[, total_users := returned[lifetime == 0], by = media_source] # альтернативный вариант # retention_stat &lt;- merge( # retention_stat, # retention_stat[lifetime == 0, list(media_source, total_users_2 = returned)], # by = &#39;media_source&#39;, all.x = TRUE # ) # считаем ретеншен retention_stat[, ret := returned / total_users] plot_ly(retention_stat, x = ~lifetime, y = ~ret, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Ретеншен по источникам пользователей&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 3 (HMP) Постройте линейный график retention 1 day (ret1) для всех дневных когорт. Т.е. по оси OX должна быть дата инсталла, по оси OY – значение ретеншена первого для пользователей, пришедших в этот день. level 4 (UV) Добавьте на этот график группировку по источникам трафика (media_source). # пересобираем датасет retention_daily &lt;- merge( installs[, list(user_pseudo_id, dt, media_source)], dau[, list(user_pseudo_id, login_dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) retention_daily[is.na(media_source), media_source := &#39;organic&#39;] retention_daily[media_source == &#39;other&#39;, media_source := &#39;organic&#39;] retention_daily[, uniqueN(user_pseudo_id), by = media_source] ## media_source V1 ## 1: applovin_int 36818 ## 2: organic 57690 ## 3: unityads_int 22017 ## 4: googleadwords_int 7774 ## 5: Facebook Ads 1297 retention_daily[, lifetime := login_dt - dt] retention_daily &lt;- retention_daily[!user_pseudo_id %in% retention_daily[lifetime &lt; 0, unique(user_pseudo_id)]] # считаем по дням инсталла вернувшихся на lifetime == 0 retention_daily_stat &lt;- merge( retention_daily[lifetime == 0, list(total_users = uniqueN(user_pseudo_id)), by = list(dt, media_source)], retention_daily[lifetime == 1, list(returned_d1 = uniqueN(user_pseudo_id)), by = list(dt, media_source)], by = c(&#39;dt&#39;, &#39;media_source&#39;), all.x = TRUE ) # считаем ретеншен retention_daily_stat[, ret1 := returned_d1 / total_users] retention_daily_stat &lt;- retention_daily_stat[order(dt)] setkey(retention_daily_stat, dt) plot_ly(retention_daily_stat, x = ~dt, y = ~ret1, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Динамика retention 1 day по дням&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 5 (N) Постройте и сравните графики rolling retention и retention rate (возьмите данные за логины и инсталлы из практикума). # считаем rolling retention # сначала вычисляем максимальную дату захода по каждому пользователю rret &lt;- retention[, list(lifetime = max(lifetime)), by = user_pseudo_id] # считаем количество дней от инсталла до последнего логина rret_stat &lt;- rret[, list(rret_users = uniqueN(user_pseudo_id)), by = lifetime] # нужна обратная кумулята, так как мы считаем &quot;сколько вернулось после дня x&quot; # а в статистике у нас &quot;для какого количества пользователей это был последний день&quot; - то есть, для каждого дня надо получить, # накопительную сумму этого и всех следующих дней. а это делается с помощью обратной кумуляты # для этого мы переворачиваем значения колонки с помощью rev(), считаем обычную кумуляту # а потом результат переворачиваем обратно # чтобы понять результат, попробуйте выражения: 1:5; rev(1:5), cumsum(1:5), cumsum(rev(1:5)), rev(cumsum(rev(1:5))) setkey(rret_stat, lifetime) rret_stat[, rret_users_cum := cumsum(rret_users)] rret_stat[, rret_users_cum_rev := cumsum(rev(rret_users))] rret_stat[, rret_users_cum_rev_rev := rev(cumsum(rev(rret_users)))] rret_stat[, rolling_ret := rret_users_cum_rev_rev / rret_users_cum_rev_rev[lifetime == 0]] # считаем простой retention без разбивки по платформам или источникам привлечения retention_stat &lt;- retention[, list(returned = uniqueN(user_pseudo_id)), keyby = list(lifetime)] retention_stat[, total_users := returned[lifetime == 0]] retention_stat[, ret := returned / total_users] # собираем обе таблицы ретеншена и рисуем plot_ly(rret_stat, x = ~lifetime, y = ~rolling_ret, type = &#39;scatter&#39;, mode = &#39;lines&#39;, name = &#39;rolling ret&#39;) %&gt;% add_trace(data = retention_stat, y = ~ret, name = &#39;retention rate&#39;) %&gt;% layout( title = &#39;Rolling retention VS Retention rate&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) Платежные метрики Gross / Net Gross - общая сумма всех платежей Revenue (или Net) - сумма платежей после вычета налогов и комиссии магазина приложений. задача 1 Нарисовать график суммы платежей по дням, с разбивкой по группам лайфтайма пользователей. # импортируем даатсает payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) # считаем количество дней от инсталла payments[, lifetime := pay_dt - install_dt] # делим на группы payments[, lifetime_group := cut(lifetime, breaks = c(-Inf, -1, 0, 7, 28, Inf), ordered_result = TRUE)] # считаем гросс payments_stat &lt;- payments[, list(gross = sum(gross)), keyby = list(pay_dt, lifetime_group)] payments_stat[, total_gross := sum(gross), by = pay_dt] payments_stat[, share := gross / total_gross] # area-plot plot_ly(payments_stat, x = ~pay_dt, y = ~gross, color = ~lifetime_group, type = &#39;scatter&#39;, mode = &#39;none&#39;, stackgroup = &#39;one&#39;) %&gt;% layout( title = &#39;Gross по группам пользователей&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) Конверсия Conversion = N Paying Users / N Users Конверсия обычно считается в каком-то “окне”, - минимальном общем количестве дней, которые могли прожить в приложении пользователи разных сегментов когорт. Например, пользователи, которые пришли месяц назад, могли платить 30 дней. Пользователи, которые пришли пять дней назад - могли платить только пять дней. Соответственно, если мы хотим сравнивать конверсию этих двух когорт, то считать надо с ограничением в пять дней - сколько пользователей первой когорты сконвертировалось за пять дней в приложении (несмотря на то, что они пришли месяц назад), и сколько пользователей второй когорты сконвертировалось за пять дней. Аналогично, когда оцениваем конверсию месячной когорты (всех пользователей, которые, например, пришли в сентябре), то надо так же считать конверсию только за какое-то определенное количество дней, чтобы не было перекосов из-за неравномерной длительности жизни пользователей в приложении. задача 2 Посчитать, какая доля пользователей, которая пришла в июне, стала платящими в интервале 30 дней от инсталла. Алгоритм 1 (не очень гибкий): - посчитать количество платящих в payments, у которых install_dt был в июне, а lifetime (дата платежа - дата инсталла) меньше или равен 30 - по таблице installs посчитать, сколько всего пришло пользователей в июне - поделить одно на другое # импортируем инсталлы installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) # создаем епременную лайфтайма payments[, lifetime := as.numeric(pay_dt - install_dt)] # выделяем сегмент пользователей payers_june &lt;- payments[install_dt &gt;= &#39;2022-06-01&#39; &amp; install_dt &lt; &#39;2022-07-01&#39;] payers_june &lt;- payers_june[lifetime &gt;= 0 &amp; lifetime &lt;= 30] payers_june[, uniqueN(user_pseudo_id)] / installs[dt &gt;= &#39;2022-06-01&#39; &amp; dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)] ## [1] 0.02343383 Алгоритм 2, тоже не очень гибкий - посчитать минимальный лайфтайм пользователей по таблице платежей - приджойнить результат к таблице инсталлов - посчитать количество пользователей всего и количество пользователей с ненулевым лайфтаймом - поделить одно на другое payers_min &lt;- payments[install_dt &gt;= &#39;2022-06-01&#39; &amp; install_dt &lt; &#39;2022-07-01&#39;] payers_min &lt;- payers_min[lifetime &lt;= 30] payers_min &lt;- payers_min[, list(min_pay_dt = min(pay_dt)), by = user_pseudo_id] jun_payers_min &lt;- merge( installs[dt &gt;= &#39;2022-06-01&#39; &amp; dt &lt; &#39;2022-07-01&#39;], payers_min, by = &#39;user_pseudo_id&#39;, all.x = TRUE ) jun_payers_min_stat &lt;- jun_payers_min[, list( total_users = uniqueN(user_pseudo_id), payers = uniqueN(user_pseudo_id[!is.na(min_pay_dt)]))] jun_payers_min_stat[, payers / total_users] ## [1] 0.02347897 задача 3 Посчитайте накопительную конверсию по когорте июньских пользователей. # ограничиваем датасет по инсталлам payments_june &lt;- payments[install_dt &lt; &#39;2022-07-01&#39;] payments_june &lt;- payments_june[, list(lifetime = min(lifetime)), by = user_pseudo_id] # и по лайфтайму payments_june &lt;- payments_june[lifetime &lt;= 30] payments_june &lt;- payments_june[lifetime &gt;= 0] # читаем количество пользователей, сделавших платеж payments_june_stat &lt;- payments_june[, list(n_payers = uniqueN(user_pseudo_id)), by = lifetime] setkey(payments_june_stat, lifetime) # кумулята и значение накопительной конверсии payments_june_stat[, payers_cum := cumsum(n_payers)] payments_june_stat[, conversion := payers_cum / installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] plot_ly(payments_june_stat, x = ~lifetime, y = ~conversion, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Накопительная конверсия в платящих&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) Полезные материалы What Is a Business Model? 30 Successful Types of Business Models You Need to Know Коротко, что такое бизнес-модели, рассматриваются 30 разных моделей. Полезно для понимания, как вообще могут зарабатывать разные продукты. Основные метрики мобильных приложений Очень обзорный материал от devtodev. Есть неплохой блок по метрикам монетизации. Домашнее задание Домашние занятия для желающих. Если будут вопросы или необходимость получить от меня какие-то комментарии - пишите в личку в slack. Задание можете выполнять на любом доступном вам языке / среде для статистики. level 1 (IATYTD) Прочитайте конспект, разберите практические занятия. Обновите знания по работе с табличками — агрегации (группировки), слияния, создание и модификация колонок. level 2 (HNTR) Постройте график накопительной конверсии с разбивкой по источнику пользователей. Датасеты: - инсталлы: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv - платежи: ‘https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv’ level 3 (HMP) Посчитайте по каждой платформе конверсию в платящих в день инсталла. Когорта – пришедшие в июне. Делать аналогично динамике ретеншена первого дня. level 4 (UV) Постройте по платформам накопительную кривую конверсии по дням от инсталла (по аналогии с накопительным ARPU). level 5 (N) Посчитайте по каждой платформе конверсию в платящих в день инсталла, суммарно на 3, 7 и 30 дни. Когорта – пришедшие в июне. Должна получиться табличка. ## platform total_users day 0 day 3 day 7 day 30 ## 1: ANDROID 77770 0.005 0.010 0.011 0.015 ## 2: IOS 33010 0.014 0.029 0.035 0.044 "],["c5_monetization.html", "Метрики монетизации pt.2 Запись занятия Разбор домашнего задания Воронка платежей", " Метрики монетизации pt.2 Запись занятия Разбор домашнего задания level 2 (HNTR) Постройте график накопительной конверсии с разбивкой по источнику пользователей. library(data.table) library(plotly) # импортируем данные installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) # перекодируем медиасорсы installs[is.na(media_source), media_source := &#39;organic&#39;] installs[media_source == &#39;other&#39;, media_source := &#39;organic&#39;] # выбираем первый инсталл installs &lt;- installs[order(user_pseudo_id, dt)] installs &lt;- installs[, .SD[1], by = user_pseudo_id] # берем платежи только новых пользователей payments_new &lt;- payments[user_pseudo_id %in% installs[, unique(user_pseudo_id)]] # прикрепляем к ним медиасорсы payments_new &lt;- merge( payments_new, installs[, list(user_pseudo_id, dt, media_source)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # payments_new &lt;- merge( # payments_new, # installs[, list(user_pseudo_id, dt, media_source)], # by = &#39;user_pseudo_id&#39;, all = FALSE # ) # вычисляем лайфтайм payments_new[, lifetime := pay_dt - dt] # корректируем на окно лайфтайма # as.Date(&#39;2022-07-31&#39;) - as.Date(&#39;2022-06-30&#39;) payments_new &lt;- payments_new[dt &lt; &#39;2022-07-01&#39;] payments_new &lt;- payments_new[lifetime &lt;= 30] payments_new &lt;- payments_new[lifetime &gt;= 0] # считаем, когда пользователь сделал первый платеж (минимальный лайфтайм) payments_new_stat &lt;- payments_new[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, media_source)] # считаем распределение пользователей по этому мин.лайфтайму payments_new_stat &lt;- payments_new_stat[, list(new_payer = uniqueN(user_pseudo_id)), by = list(media_source, lifetime)] # сортируем и делаем кумулятивную сумму payments_new_stat &lt;- payments_new_stat[order(media_source, lifetime)] payments_new_stat[, new_payer_cum := cumsum(new_payer), by = media_source] # считаем, сколько всего пользователей пришло installs_stat &lt;- installs[, list(total_users = uniqueN(user_pseudo_id)), by = media_source] payments_new_stat &lt;- merge( payments_new_stat, installs_stat, by = &#39;media_source&#39;, all.x = TRUE ) payments_new_stat[, conversion := new_payer_cum / total_users] # рисуем plot_ly(payments_new_stat, x = ~lifetime, y = ~conversion, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;накопительная конверсия по источникам трафика&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 3 (HMP) Посчитайте по каждой платформе конверсию в платящих в день инсталла. Когорта – пришедшие в июне. Делать аналогично динамике ретеншена первого дня. # аналогично предыдущему заданию # пересобираем датасет, так как в прошлом задании мы в нем делали другие вычисления # и он нужен заново payments_new &lt;- payments[user_pseudo_id %in% installs[, unique(user_pseudo_id)]] payments_new &lt;- merge( payments_new, installs[, list(user_pseudo_id, dt, media_source)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) payments_new[, lifetime := pay_dt - dt] payments_new &lt;- payments_new[dt &lt; &#39;2022-07-25&#39;] payments_new &lt;- payments_new[lifetime &lt;= 7] payments_new &lt;- payments_new[lifetime &gt;= 0] # отличие от предыдущего задания -- делаем группировку не по медиасорсам, а по дате инсталла payments_new_stat &lt;- payments_new[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, dt)] payments_new_stat &lt;- payments_new_stat[, list(new_payer = uniqueN(user_pseudo_id)), by = list(dt, lifetime)] installs_stat &lt;- installs[, list(total_users = uniqueN(user_pseudo_id)), by = dt] payments_new_stat &lt;- merge( payments_new_stat, installs_stat, by = &#39;dt&#39;, all.x = TRUE ) payments_new_stat &lt;- payments_new_stat[order(dt, lifetime)] payments_new_stat[, new_payer_cum := cumsum(new_payer), by = dt] payments_new_stat[, conversion := new_payer_cum / total_users] # оставляем только накопительную конверсию в платящих на определенные дни от инсталла payments_new_stat &lt;- payments_new_stat[lifetime %in% c(0, 1, 3, 7)] plot_ly(payments_new_stat[dt &lt; &#39;2022-07-01&#39;], x = ~dt, y = ~conversion, color = ~as.character(lifetime), type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Динамика конверсии пользователей по дням&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Воронка платежей Доля пользователей, которые сделали второй, третий и т.д. платеж. # опять пересобираем исходную табличку платежей payments_new &lt;- payments[user_pseudo_id %in% installs[, unique(user_pseudo_id)]] payments_new &lt;- merge( payments_new, installs[, list(user_pseudo_id, dt, media_source)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # отсортировать платежи пользователей по возрастанию payments_new &lt;- payments_new[order(user_pseudo_id, ts)] # альтернативные методы сортировки # setkey(payments_new, user_pseudo_id, ts) # setorder(payments_new, user_pseudo_id, ts) # создаем номер платежа каждого пользователя. 1:.N -- и так в группе по пользователям payments_new[, purchase_number := 1:.N, by = user_pseudo_id] # payments_new[, purchase_number := seq(1, .N, by = 1), by = user_pseudo_id] payments_new[, lifetime := pay_dt - dt] payments_new &lt;- payments_new[dt &lt; &#39;2022-07-01&#39;] payments_new &lt;- payments_new[lifetime &lt;= 30] payments_new &lt;- payments_new[lifetime &gt;= 0] # считаем, сколько пользователей сделало платеж с этим номером payments_funnel &lt;- payments_new[, list(n_users = uniqueN(user_pseudo_id)), keyby = purchase_number] # считаем посчитать долю от всего пользователей, сделавших платеж (purchase_number == 1) payments_funnel[, total_payers := n_users[purchase_number == 1]] # если у нас есть группировка, то надо отдельно считать и мерджить по ключу # рисуем payments_funnel[, share := n_users / total_payers] plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~share, type = &#39;bar&#39;) %&gt;% layout( title = &#39;Воронка платежей&#39; ) %&gt;% config(displayModeBar = FALSE) Воронки можно считать не от первого шага, а от предыдущего. В некоторых случаях это удобнее и информативнее. # если хотим считать от предыдущего шага payments_funnel[, prev_users := shift(n_users, n = 1)] payments_funnel[, prev_share := n_users / prev_users] plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~prev_share, type = &#39;bar&#39;) %&gt;% layout( title = &#39;Воронка платежей, доля от предыдущего&#39; ) %&gt;% config(displayModeBar = FALSE) ## Warning: Ignoring 1 observations plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~share, type = &#39;bar&#39;, name = &#39;% from payers&#39;) %&gt;% add_trace(y = ~prev_share, name = &#39;% from prev&#39;) %&gt;% layout( title = &#39;Воронка платежей&#39; ) %&gt;% config(displayModeBar = FALSE) ## Warning: Ignoring 1 observations "],["метрики-монетизации-pt.html", "Метрики монетизации pt.3 Запись занятия ARPU / ARPPU LTV Полезные материалы", " Метрики монетизации pt.3 Запись занятия ARPU / ARPPU Averange revenue per user - сумма платежей за определенный период, деленная на общее количество пользователей когорты. Средний чек, наверное, одна из самых важных метрик для оперирования продуктом, так как изучение структуры ARPU позволяет понять, за что платят пользователи и как можно улучшить эту метрику и так далее. Average revenue per paying user - сумма платежей за определенный период, деленная на количество платящих пользователей когорты. Обе метрики считаются в определенном окне (количестве дней от инсталла) - обычно 7, 28 или 30 дней. Это необходимо для того, чтобы учесть ситуацию, когда пользователи одной когорты (месячной, например) могли прожить разное количество дней в приложении. Или когда необходимо сравнить разные каналы привлечения, рекламные кампании или группы аб-тестов. задача 1 Считаем статистики в окне 7 дней от инсталла: library(data.table) library(plotly) library(kableExtra) # импортируем данные installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) # перекодируем медиасорсы installs[is.na(media_source), media_source := &#39;organic&#39;] installs[media_source == &#39;other&#39;, media_source := &#39;organic&#39;] # выбираем первый инсталл installs &lt;- installs[order(user_pseudo_id, dt)] installs &lt;- installs[, .SD[1], by = user_pseudo_id] # берем платежи только новых пользователей payments_new &lt;- payments[user_pseudo_id %in% installs[, unique(user_pseudo_id)]] # прикрепляем к ним медиасорсы payments_new &lt;- merge( payments_new, installs[, list(user_pseudo_id, dt, media_source)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # вычисляем лайфтайм payments_new[, lifetime := pay_dt - dt] # корректируем на окно лайфтайма payments_new &lt;- payments_new[dt &lt; &#39;2022-07-01&#39;] payments_new &lt;- payments_new[lifetime &lt; 7] # отсюда надо считать количество денег и платящих # по сути, просто группировка по медиасорсу payments_new_stat &lt;- payments_new[, list( payers_7 = uniqueN(user_pseudo_id), gross_7 = sum(gross), n_purchases = .N ), by = media_source] # считаем, сколько всего пользователей пришло installs_stat &lt;- installs[, list(total_users = uniqueN(user_pseudo_id)), by = media_source] payments_new_stat &lt;- merge( installs_stat, payments_new_stat, by = &#39;media_source&#39;, all.x = TRUE ) payments_new_stat[, ARPU_7 := round(gross_7 / total_users, 2)] payments_new_stat[, ARPPU_7 := round(gross_7 / payers_7, 2)] payments_new_stat[, conv_7 := round(payers_7 / total_users, 3)] payments_new_stat[, avg_purchase := round(gross_7 / n_purchases, 1)] payments_new_stat[, purchases_per_user := round(n_purchases / payers_7, 1)] payments_new_stat ## media_source total_users payers_7 gross_7 n_purchases ARPU_7 ARPPU_7 ## 1: Facebook Ads 1297 40 612.47 64 0.47 15.31 ## 2: applovin_int 36818 878 17352.16 2161 0.47 19.76 ## 3: googleadwords_int 7774 122 2092.53 247 0.27 17.15 ## 4: organic 57690 753 16699.53 1968 0.29 22.18 ## 5: unityads_int 22017 206 2990.22 477 0.14 14.52 ## conv_7 avg_purchase purchases_per_user ## 1: 0.031 9.6 1.6 ## 2: 0.024 8.0 2.5 ## 3: 0.016 8.5 2.0 ## 4: 0.013 8.5 2.6 ## 5: 0.009 6.3 2.3 # kable_classic(kbl(payments_new_stat)) # kable_material(kbl(payments_new_stat)) LTV Lifetime value - общая сумма платежей, которые сделает пользователь за всю свою жизнь в приложении. Так как для каждого пользователя обычно сложно предсказать, сколько он проживет, то считается как кумулятивный средний чек когорты - общая накопленная сумма платежей, сделанная к определенному дню от инсталла, деленная на количество пользователей когорты. Кривая LTV/cumARPU сходится к общему значению ARPU по всей выборке за все время жизни когорты. LTV, фактически, одна из ключевых метрик, так как график LTV/cumARPU позволяет оценить динамику платежей когорты, сделать какие-то предсказания. Отношение LTV и CPI позволяют оценить эффективность рекламных кампаний. Например, за 90 дней от инсталла платежами пользователей возвращается 40-60% затраченных на привлечение денег. На 270-360 - все затраченные, и дальнейшие платежи составляют чистую прибыль (абстрактный пример, в реальности периоды сильно зависят от продукта). Соответственно, если даже в перспективе значение LTV когорты (сколько заплатит каждый привлеченный пользователь) не превысит CPI (сколько стоило привлечение каждого пользователя) когорты, то такая рекламная кампания убыточна и может быть полезна только для увеличение объема пользователей. задача 2 Посчитать кумулятивное ARPU 30 дня по июньской когорте Алгоритм - посчитать сумму гросса по дням лайфтайма - посчитать кумулятивную сумму этого гросса c помощью cumsum() - поделить кумулятивную сумму на общее количество пользователей когорты - нарисовать график # пересоздаем датасет, так как раньше отрезали только 7 дней # берем платежи только новых пользователей payments_new &lt;- payments[user_pseudo_id %in% installs[, unique(user_pseudo_id)]] # прикрепляем к ним медиасорсы payments_new &lt;- merge( payments_new, installs[, list(user_pseudo_id, dt, media_source)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # вычисляем лайфтайм payments_new[, lifetime := pay_dt - dt] # корректируем на окно лайфтайма payments_new &lt;- payments_new[dt &lt; &#39;2022-07-01&#39;] payments_new &lt;- payments_new[lifetime &lt;= 30] payments_new &lt;- payments_new[lifetime &gt;= 0] # считаем платежи payments_new_stat &lt;- payments_new[, list(gross = sum(gross)), by = list(media_source, lifetime)] # сортируем и делаем кумулятивную сумму payments_new_stat &lt;- payments_new_stat[order(media_source, lifetime)] payments_new_stat[, gross_cum := cumsum(gross), by = media_source] # считаем, сколько всего пользователей пришло installs_stat &lt;- installs[, list(total_users = uniqueN(user_pseudo_id)), by = media_source] payments_new_stat &lt;- merge( payments_new_stat, installs_stat, by = &#39;media_source&#39;, all.x = TRUE ) payments_new_stat[, cumARPU := gross_cum / total_users] # рисуем plot_ly(payments_new_stat, x = ~lifetime, y = ~cumARPU, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;cARPU по источникам трафика&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Полезные материалы What Is a Business Model? 30 Successful Types of Business Models You Need to Know Коротко, что такое бизнес-модели, рассматриваются 30 разных моделей. Полезно для понимания, как вообще могут зарабатывать разные продукты. Основные метрики мобильных приложений Очень обзорный материал от devtodev. Есть неплохой блок по метрикам монетизации. "],["unit-экономика.html", "Unit-экономика Запись занятия Unit-экономика Дополнительные материалы", " Unit-экономика Запись занятия Unit-экономика Что это и зачем Юнит-экономика – определение числа юнитов масштабирования, маржинальная прибыль от которых необходима для покрытия постоянных издержек и выхода на заданный уровень прибыли. Основная идея юнит-экономики простая: Прибыль = маржинальная прибыль – постоянные расходы. Главное, как формируется маржинальная прибыль – количество клиентов, расходы на привлечение и удержание, Юнит-экономику можно считать как до запуска нового предприятия, так и после. Например, если мы хотим запустить стартап, то расписанная юнит-экономика стартапа нужна инвесторам, чтобы понимать за счет чего и когда будут окупаться вложения, насколько вообще жизнеспособна идея. Точно также юнит-экономику можно считать в уже существующем бизнесе, чтобы определить возможные зоны роста и слабые места. Стоит помнить, что расписанная юнит-экономика проекта — это всего лишь модель. А реализуют ее люди, и многие вещи будут зависеть не от модели или рынка, а от команды проекта. Юниты масштабирования Юнитом масштабирования может быть что угодно – одна сделка, одна продажа, один заказчик или подписчик. Юниты принято выбирать в соответствии с целями компании: что конкретно мы собираемся масштабировать. Юниты могут иметь разную иерархию, на примере пиццерии: заказ &gt; клиент &gt; пиццерия &gt; город &gt; страна. Точно также в рамках одной (крупной) компании может быть несколько разных юнитов. Сложностью юнита масштабирования добавляет новые метрики в формулы окупаемости и, в целом, рычагов ее изменеяия. Например, если мы продаем какой-то физический предмет, то нас интересует в первую очередь себестоимость товара. Если наш юнит – клиент, то мы смотрим на LTV и в модель закладываются расходы на удержание клиента, на повышение среднего чека, и т.д. Продуктовые аналитики обычно работают с проектами, где юнит масштабирования – клиент, именно поэтому наши ключевые метрики – ARPU, LTV, конверсия в платеж, среднее количество платежей на пользователя и т.д. Основные термины Fix Costs (Фиксированные расходы): расходы, не зависящие линейно от объема продаж или производства. Например, зарплата генерального директора или программиста в продуктовой компании. Variable Costs (Переменные расходы): расходы, непосредственно привязанные к единице товара или услуги. Например, стоимость сырья или доставка. В моделях обычно раскладывается на расходы на привлечение / маркетинг, удержание пользователя (Aquisition costs), себестоимость товара или услуги (COGS) и т.д. COGS (Cost of Goods Sold): Себестоимость единицы товара или услуги — стоимость материалов, логистики, оплата работы. В общем виде — переменные издержки, которые несет бизнес в момент сделки. Наивное правило, с помощью которого можно определить, относятся те или иные расходы к COGS или нет: если ваши расходы равны нулю, если у вас нет сделок, то это COGS, и наоборот – если не равны нулю (например, аренда или заработная плата), то это не COGS. First sale COGS (1sCOGS, Начальные расходы): Дополнительные переменные издержки, которые несет бизнес в момент самой первой сделки. Сюда же можно отнести расходы на первом этапе запуска проекта (закупка оборудования, пуско-наладочные работы и т. д.). Gross Profit (Валовая прибыль): разница между ценой товара или услуги и её себестоимостью. Нередко называют просто маржой. Если мы считаем не абсолютные значения (деньги), а долю от выручки, то это маржинальность, Gross Profit Margin. User/Unit (Пользователь): базовая сущность, определяет, с чем мы работаем, в общем случае представляет собой человека, который познакомился с продуктом благодаря рекламе. User / Lead Acqusition: количество привлекаемых пользователей. По сути горизонт масштабирования, если юнит масштабирования — пользователи. Aquisition costs (AC, Расходы на привлечение пользователей): расходы на рекламу, маркетинг, промокоды и другие сопутствующие расходы, на препродажи (если есть). AC = User Acquisition × Cost per Acquisition = Buyer × Customer Acquisition Cost CAC (Customer Acquisition Cost, Стоимость привлечения пользователя): расходы на привлечение одного платящего пользователя. То есть пользователя, который купил товар или услугу. Cost per Acquisition (CPA, Расходы на привлечение пользователя): стоимость привлечения пользователя, независимо, сделал он платеж или нет. В наиболее общем виде — стоимость маркетинговых затрат на юнит масштабирования. Наиболее характерные метрики: Cost per Install (CPI), Cost per Action (CPS), Cost per Lead (CPL). Сюда же можно отнести промежуточные метрики рекламных кампаний: Cost per Mile (CPM, цена за тысячу показов рекламы), CPC (Cost per Click, цена за количество кликов по рекламе) и прочие. Conversion to first purchase (C1, Конверсия в первую покупку): какой процент привлеченных юнитов масштабирования становится клиентами. Одна из ключевых метрик продукта, определяет, насколько хорошо продукт продает ценность. Buyer/Customer (B, клиент / пользователь): число клиентов, которых компания получает от потока пользователей с учетом имеющегося коэффициента конверсии. то есть это тот user / unit, который сделал покупку или воспользовался услугой. B = UA × C1. Average Payment Count (APC): Среднее число платежей, приходящееся на одного клиента. APC = Количество платежей / Buyer. Правда, надо учитывать интервал, за который сделаны платежи. Average Order Value (AOV или Av. Price, Средний чек): сумма, которую в среднем платят клиенты за товары или услуги. Customer Lifetime Value (CLTV): доход на одного платящего пользователя (клиента). Нюанс в том, что мы обычно не знаем, за какой период берутся платежи пользователя. Максимальный период – Lifetime, но в практике обычно берут 90/180/350 и более дней, в зависимости от бизнеса. CLTV = (Av. Price – COGS) × APC – 1sCOGS Contribution Margin (CM, Маржинальная прибыль): разница между тем, сколько мы получили от продажи товаров/услуг привлеченным пользователям и стоимостью привлечения пользователей. Можно выразть как Gross Profit - Acquisition Cost. Для достижения точки безубыточности маржинальная прибыль должна покрывать постоянные издержки. При положительном значении маржинальной прибыли говорят, что юнит-экономика сходится. А положительную разницу между маржинальной прибылью и постоянными издержками можно принять за EBITDA. Пример Общая идея бизнеса: покупаем стулья в Китае, привозим в Россию и продаем дистрибьютору. Предположим, продаем мы один стул за 1000 рублей (Av. Price). При этом наши переменные расходы (COGS) на единицу товара: 60 рублей налог (6% с оборота) 250 рублей закупка 100 рублей растаможка 150 рублей логистика 100 рублей брак (вероятность 10%) 40 рублей остальное Рекламы у нас нет, так как мы продаем стулья дистрибьютору. Поэтому Acquisition cost = 0. В результате с каждого стула у нас маржа (Gross Profit) 300 рублей. При этом мы не учитываем постоянные расходы на зарплату и аренду офиса/склада. Таким образом за 100 стульев мы получим 30000 рублей. Если вычесть постоянные расходы, то, скорее всего, мы будем в убытке. Однако если мы решим привезти 10000 стульев, то маржа вырастет, а переменные расходы останутся теми же. И мы, возможно, выйдем в прибыль. Шаблоны расчета Рекомендую поизучать шаблоны расчета юнит-экономики Ильи Красинского. Это один евангелистов юнит-экономики в ру-сегменте. У него достаточно подробный набор метрик, как общих, так и на пользователя. Точно также есть другие шаблоны, тысячи их: Шаблон юнит-экономики Яндекс Практикум Шаблон расчета юнит экономики (Skillbox) Симулятор стартапа Unit-экономика для аналитиков Юнит-экономика – в первую очередь инструмент для оценки бизнеса, поиска его узких точек и возможностей масштабирования. Продуктовые аналитики касаются только небольшой ее части — в первую очередь того, что касается поведения пользователя (то, как он платит, за что платит, как долго пользуется, как возвращается, как удерживается и т. д.), во вторую очередь — затраты на привлечение пользователя. И то в общем виде, сколько потратили на привлечение, так как более тонкими деталями типа CTR, IR, спецификой каналов привлечения, маркетинговой атрибуцией и т. д. занимаются больше маркетинговые аналитики. Постоянный расходы типа оплаты аренды офиса, ФОТ, налоги, прочие расходы и доходы, не связанные конкретно с клиентами/пользователями обычно остаются не в фокусе или вообще за пределами задач продуктовых аналитиков. Собственно, задача продуктовых аналитиков — тем или иным способом увеличить деньги от пользователя и таким образом повысить маржинальную прибыль и/или окупаемость. Дополнительные материалы учебник Д.Ханина по юнит-экономике. видео-курс Д.Ханина по юнит-экономике платный курс А.Горного “Unit-экономика от А до Я” конспект по юнит-экономике от Я.Практикума неплохая инфографика по юнит-экономике неплохая статья в блоке Контур.Компас "],["ux.html", "UX intro Запись занятия Полезные ссылки", " UX intro Запись занятия Полезные ссылки Телеграм-сообщество UX REsearch. Регулярная подборка лучших постов про UX-исследования и смежных областей. Конференция ПрофсоUX у них есть записи предыдущих конференций. Недавняя конференция UXlab Mail.ru. Много кейсов, выступают исследователи UX-лаборатории. Канал о личном опыте маркетинговых и продуктовых исследований: нетривиальных случаях, труднодоступных аудиториях и работающих методах. Customer Development и Custdev. Что это такое и в чем разница? Статья в блоге Олега Якубенкова. "],["sql.html", "SQL Запись занятия DBeaver Параметры подключений SQL r connectors Полезные ссылки", " SQL Запись занятия DBeaver https://dbeaver.io/download/ Параметры подключений user = “student” password = “pmsar2018” dbname = “pmsar” host = “188.225.77.95” port = 5432 SQL main structure SQL - декларативный язык с жестко закрепленным порядком операторов (зарезервированных ключевых слов) в sql-выражении. При запросе данных из таблицы обязательны select и from, остальные - по необходимости. select (указание, какие колонки таблицы должны быть в результате, аналог j в data.table- dt[i, j, by]) from (из какой таблицы или результата слияния таблиц должны быть выбраны колонки) join (какая таблица должна быть присоединена по ключу, аналог merge в R) where (набор логических выражений для фильтрация по строкам, аналогично фильтрации в data.table в разделе i - dt[i, j, by]) group by (по значениям каких колонок должна быть группировка, аналог by в data.table - dt[i, j, by]) order by (по значениям каких колонок должна быть отсортированная результирующая таблица) limit (ограничение на выдачу, сколько строк таблицы должно быть отдано) ; (завершение запроса, некоторые IDE могут за пользователя подставлять) simple query Простейшие арифметические запросы требуют select и все: select 1 + 5; Table 1: 1 records ?column? 6 select now(); Table 2: 1 records now 2023-12-05 00:08:38 select: columns selection Запросы для выбора колонок. * используется как аналог select all, то есть выбор всех колонок, которые есть в таблице. В разделе from указывается схема (набор таблиц) и таблица из этой схемы, через точку: public.chars означает таблица chars из схемы public: select * from public.chars limit 3; Table 3: 3 records row.names name height mass hair_color skin_color eye_color birth_year gender url planet_name 1 Luke Skywalker 172 77 blond fair blue 19BBY male https://swapi.co/api/people/1/ Tatooine 2 C-3PO 167 75 n/a gold yellow 112BBY n/a https://swapi.co/api/people/2/ Tatooine 3 Darth Vader 202 136 none white yellow 41.9BBY male https://swapi.co/api/people/4/ Tatooine Также в блоке select можно указать одну или несколько колонок, результат арифметических операций над колонками. select name, height, planet_name from public.chars limit 3; Table 4: 3 records name height planet_name Luke Skywalker 172 Tatooine C-3PO 167 Tatooine Darth Vader 202 Tatooine Алиасы формы column_name as new_name используются для переименования колонок или результатов вычислений. select name as char_name, height, planet_name, height * 3 as height_mult from public.chars limit 3; Table 5: 3 records char_name height planet_name height_mult Luke Skywalker 172 Tatooine 516 C-3PO 167 Tatooine 501 Darth Vader 202 Tatooine 606 where: rows selection Для фильтрации по строкам используют набор логических выражений в разделе where. where planet_name = 'Coruscant' читается как все строки, в которых в колонке planet_name встречается значение Coruscant. Строковые значения и даты указыаются в одинарных кавычках, двойные кавычки только для названий колонок и схем. select name, height, planet_name from public.chars where planet_name = &#39;Coruscant&#39;; Table 6: 3 records name height planet_name Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant Логические выражения можно сочетать через операторы and и or, они аналогичны логическим операторам &amp; и | в R. select name, height, planet_name from public.chars where planet_name = &#39;Coruscant&#39; or height &gt;= 170; Table 7: Displaying records 1 - 10 name height planet_name Luke Skywalker 172 Tatooine Darth Vader 202 Tatooine Owen Lars 178 Tatooine Biggs Darklighter 183 Tatooine Anakin Skywalker 188 Tatooine Cliegg Lars 183 Tatooine Boba Fett 183 Kamino Lama Su 229 Kamino Taun We 213 Kamino Poggle the Lesser 183 Geonosis Оператор in аналогичен оператору %in% в R. Отрицание производится как not in: select name, height, planet_name from public.chars where planet_name in (&#39;Coruscant&#39;, &#39;Alderaan&#39;); Table 8: 6 records name height planet_name Leia Organa 150 Alderaan Bail Prestor Organa 191 Alderaan Raymus Antilles 188 Alderaan Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant Также есть инструментарий проверки на вхождение, аналог grepl() в R. Для этого используется оператор like, а в качестве искомого выражения указывается строка, где % используются в качестве любые символы. Выражение planet_name like '%Coru%' можно прочитать как “все строки таблицы, в которых в колонке planet_name встречаются строковые значения, содержащие ‘Coru’, притом и до, и после ‘Coru’ могут быть еще символы”“. Отрицание также делается как not like, для регулярных выражений используется оператор ~: select name, height, planet_name from public.chars where planet_name like &#39;%Coru%&#39; or planet_name ~ &#39;raan&#39;; Table 9: 6 records name height planet_name Leia Organa 150 Alderaan Bail Prestor Organa 191 Alderaan Raymus Antilles 188 Alderaan Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant group by: aggregations Группировки аналогичны группировкам в data.table - для каждой группы строк, выделяемых по значениям группирующей колонки, над колонками производятся вычисления (средние, суммы и проч). В примере ниже считается количество строк в таблице персонажей для каждого значения, количество уникальных персонажей (в таблице одна строка на персонажа, так что совпадает с предыдущим значением) и максимальный рост среди персонажей этой группы (всех персонажей с этой планеты): select planet_name, count(*) as n_rows, count(distinct name) as n_unique_chars, max(height) as max_height from public.chars group by planet_name limit 10; Table 10: Displaying records 1 - 10 planet_name n_rows n_unique_chars max_height Alderaan 3 3 191 Aleen Minor 1 1 79 Bespin 1 1 175 Bestine IV 1 1 180 Cato Neimoidia 1 1 191 Cerea 1 1 198 Champala 1 1 196 Chandrila 1 1 150 Concord Dawn 1 1 183 Corellia 2 2 180 join Слияние таблиц по ключу, в R аналогом выступает merge(). В зависимости от схемы присоединения, используются разные операторы джойна, чайще сего - left join (сохраняем все значения ключевой колонки в той таблице, к которой присоединяем) и inner join (сохраняем только те строки, по которым значения есть и в той таблице, к которой присоединяем, и в которой присоединяем). При этом в блоке select указываем те колонки, которые хотим получить из результата слияния. Если в таблицах используются одни и те же назания колонок, то колонки надо указывать с указанием таблицы или алиаса таблицы - table1.column_name. В разделе from указывается таблица, к которой присоединяется вторая или следующие таблицы. По возможности это должна быть самая короткая таблица. Ключ, по которому соединяются колонки - using(column_name). В том случае, когда в разных таблицах колонка-ключ называется по-разному, можно использовать выражение on table1.column_name1 = table2.column_name2. Где table1 и table2 - назания таблиц и могут быть заменены алиасами таблиц. В редких случаях в конструкции с on можно использовать знаки сравнения, чтобы фильтровать определенные значения, это может ускорять выполнение запроса, но стилистически это лучше делать в разделе where. Если через join присоединяется несколько таблиц, то они присоединяются не последовательно, к результату предыдущего джойна, а к таблице, указанной в from - то есть, порядок джойнов значения не имеет. В том случае, если используется внутренний селект (в выражении join указывается не таблица, а подзапрос, с select, from и прочими атрибутами), то для таким образом полученной таблицы нужно указать алиас. select name, height, skin_color, climate, gravity, terrain, ch.url as char_url, pl.url as planet_url from public.chars as ch left join ( select * from public.planets limit 10) as pl using(planet_name) order by name limit 5; Table 11: 5 records name height skin_color climate gravity terrain char_url planet_url Ackbar 180 brown mottle NA NA NA https://swapi.co/api/people/27/ NA Adi Gallia 184 dark temperate 1 standard cityscape, mountains https://swapi.co/api/people/55/ https://swapi.co/api/planets/9/ Anakin Skywalker 188 fair NA NA NA https://swapi.co/api/people/11/ NA Ayla Secura 178 blue NA NA NA https://swapi.co/api/people/46/ NA Bail Prestor Organa 191 tan temperate 1 standard grasslands, mountains https://swapi.co/api/people/68/ https://swapi.co/api/planets/2/ Визуальная схема вариантов джойнов (для merge() тоже полезно, для понимания аргументов all.x, all.y) r connectors Для подключения из R используется пакет RPostgreSQL. С помощью функции dbConnect(), куда аоргументами передаются параметры подключения, создается объект-коннектор. library(RPostgreSQL) con &lt;- dbConnect(PostgreSQL(max.con = 100), user = &quot;student&quot;, password = &quot;pmsar2018&quot;, dbname = &quot;pmsar&quot;, host = &quot;188.225.77.95&quot;, port = 5432) Запросы же делаются с помощью функции dbGetQuery() (простая функция для селектов, есть также отдельные функции для операций над таблицами). Первый аргумент функции - объект-коннектор, второй - строковая запись запроса. Нередко удобнее строку запроса записывать в отдельный объект: query &lt;- &quot; select name, height, planet_name from public.chars limit 3 &quot; res &lt;- dbGetQuery(conn = con, statement = query) res ## name height planet_name ## 1 Luke Skywalker 172 Tatooine ## 2 C-3PO 167 Tatooine ## 3 Darth Vader 202 Tatooine Результат выполнения выражения - объект класса data.frame. То есть, его в дальнейшем желательно переконвертировать в data.table: class(res) ## [1] &quot;data.frame&quot; После завершения работы (в идеале-после каждого запроса) соединение с базой надо закрывать: dbDisconnect(con) ## [1] TRUE Подключение и просмотр БД из RStudio В последних версиях RStudio реализовано подключение к базам данных, просмотр таблиц и вызов чистых sql-запросов - аналогично DBeaver. Для этого необходимо установить пакет odbc и соответствующие драйверы подключений баз данных (подробнее см. здесь). Само создание подключения к PosgreSQL-базе данных выглядит аналогично подключению через пакет RPostgreSQL: con_odbc &lt;- DBI::dbConnect(odbc::odbc(), # sudo apt-get install odbc-postgresql Driver = &quot;PostgreSQL Unicode&quot;, Server = &quot;188.225.77.95&quot;, Database = &quot;pmsar&quot;, UID = &quot;student&quot;, PWD = &quot;pmsar2018&quot;, Port = 5432) При этом в окне Connections (обычно в верхней правой части полей RStudio, где в Environment перечислены объекты в рабочем пространстве) появляются подключение и схемы базы данных, к которой совершено подключение. Схемы - раскрывающиеся списки, при клике выводятся все таблицы схемы. После того, как в RStudio произведено подключение таким образом, можно писать запросы в отдельных скриптах - для этого необходимо создать скрипт с расширением sql (например, File &gt; New file &gt; SQL script), а в первой строчке указать подключение, и потом нажать Preview или Ctrl+Shift+Enter: Полезные ссылки Гайды хорошего оформления sql-кода. Им необязательно следовать дословно, но все же желательно принимать во внимание. ТАк или иначе, самое главное - код должен быть лаконичным, опрятным и читабельным для коллег. style guide - я предпочитаю такой гайд, хотя он во многом может вызывать нарекания. В частности, операторы многие пишут заглавными буквами, так как это повышает их видимость в коде. Также в этом гайде критикуется разное выравнивание ключевых слов и названий таблиц (чтобы формировался “коридор”). второй гайд, один из моих коллег старается ему следовать, например. Особенно осмысленно выглядит критика префиксов в названиях колонок. список онлайн-курсов по SQL на DataCamp. Курсов, может быть, больше чем надо для реальной работы, но пройти базовые вполне можно. русскоязычный сайт-соревнование по решению задачек на SQL, некоторые задачки могут быть достаточно хардкорны. еще один неплохой ресурс с задачками по PostgreSQL. На самом деле онлайн-учебников и курсов очень много. "],["sql-advanced.html", "SQL advanced Запись занятия Части SQL data types values subquery Common tables expressions select experiments Window functions explain", " SQL advanced Запись занятия Части SQL Data Definition Language Набор команд для работы с объектами базы данных. С помощью этих команд можно создать, удалить или изменить какой-нибудь объект: таблицу, схему, функцию и т. д. Как правило для этих команд требуются дополнительные права пользователей. CREATE – для создания объектов базы данных ALTER – для изменения объектов базы данных DROP – для удаления объектов базы данных Data Manipulation Language Набор команд для работы с данными - выбор данных (из таблицы или таблиц), добавление или изменение данных, удаление данных (обычно строк из таблицы, сама таблица при этом остается на месте - для ее удаления нужно сделать DROP TABLE). SELECT – выборка данных INSERT – добавляет новые данные UPDATE – изменяет существующие данные DELETE – удаляет данные Data Control Language Организация и контроль над доступом к базе данных. Например, службе дашборда надо выдать права на чтение данных. GRANT – предоставляет пользователю или группе разрешения на определённые операции с объектом REVOKE – отзывает выданные разрешения DENY – задаёт запрет, имеющий приоритет над разрешением Transaction Control Language Команды для работы с транзакциями (группами запросов, которые выполняются пакетно, чтобы не было неконсистетности в данных). Обычно аналитики с такими задачами не сталкиваются. BEGIN – служит для определения начала транзакции COMMIT – применяет транзакцию ROLLBACK – откатывает все изменения, сделанные в контексте текущей транзакции SAVEPOINT – устанавливает промежуточную точку сохранения внутри транзакции data types list В PostgreSQL (как и в других диалектах) есть большой набор разных типов данных, от стандартных (целые числа, с дробной частью, с плавающей точкой, строки, даты) до экзотических типа ip-адресов. Подробный список типов можно посмотреть вот здесь: 8. Data Types: 8.1. Numeric Types 8.2. Monetary Types 8.3. Character Types 8.4. Binary Data Types 8.5. Date/Time Types 8.6. Boolean Type 8.7. Enumerated Types 8.8. Geometric Types 8.9. Network Address Types 8.10. Bit String Types 8.11. Text Search Types 8.12. UUID Type 8.13. XML Type 8.14. JSON Types 8.15. Arrays 8.16. Composite Types 8.17. Range Types 8.18. Domain Types 8.19. Object Identifier Types 8.20. pg_lsn Type 8.21. Pseudo-Types Numeric types При работе с числовыми типами надо помнить о такой особенности, что целые числа и числа с дробью - это разные типы. И, например, при делении целого числа на целое SQL вернет также целое число (в R будет неявное преобращование типа): select 1 / 7, 7 / 3 Table 1: 1 records ?column? ?column? 0 2 Один из самых простых вариантов явного преобразования - умножить целое число с типом numeric (то есть, на 1.000): select 1 * 1.000 / 7 Table 2: 1 records ?column? 0.1428571 Character Types Строковые типы, такие же как и в других языках программирования. select &#39;abc&#39; Table 3: 1 records ?column? abc Из полезных функций - конкатенация (слияние строк, аналог paste0() в R) и изменение регистра. select &#39;a&#39; || &#39;b&#39;, upper(&#39;abc&#39;), lower(&#39;ABC&#39;) Table 4: 1 records ?column? upper lower ab ABC abc Для работы со строковыми данными есть большая группа функций, использующих регулярные выражения. Вообще, регулярные выражения - весьма часто встречающаяся в жизни аналитиков вещь и их стоит освоить. Date/Time Types Даты и время. Несмотря на то, что для для людей более читабельны даты и время в ISO-представлении (‘гггг-мм-дд чч:мм:сс’), лучше использовать unix-timestamp – представление даты в виде количества секнуд с 1970-01-01. Это представление проще, удобнее для хранения, не зависит от таймзоны пользователя и базы данных. --текущая дата select current_date Table 5: 1 records date 2023-12-05 Для преобразования даты в unix-timestamp используют функцию extract() с указанием, что извлекается epoch. select extract(epoch from current_date) Table 6: 1 records date_part 1701734400 Обратное преобразование с помощью to_timestamp(): select to_timestamp(1636156800) Table 7: 1 records to_timestamp 2021-11-06 03:00:00 Даты вычитать достаточно просто, date - date. Но если надо из даты вычесть количество дней / месяцев / лет (или другой интервал), то можно воспользоваться следующей конструкцией: select current_date - interval &#39;1&#39; day Table 8: 1 records ?column? 2023-12-04 type Conversion Для преобразования типов в Postgresql обычно используют ::, также есть более классическая и распространенная во всех диалектах функция cast(): select 1 * 1.000 / 7, 1 :: numeric / 7, cast(1 as numeric) / 7 Table 9: 1 records ?column? ?column? ?column? 0.1428571 0.1428571 0.1428571 values Иногда бывают ситуации, когда надо создать таблицу в запросе - для этого можно с помощью команды values вычислить набор строк, в которых заданы значения (количество значений в строках должно быть одинаковыми). Названия колонок в создаваемой таблице можно задать с помощью as tablename(col1_name, col2_name…), по количеству создаваемых колонок. select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) Table 10: 2 records var1 var2 var3 1 a grp1 2 b grp1 subquery Нередко в запросах надо обратиться к подвыборке из другой таблицы. Например, это может быть как в разделе join: select * -- создаем и обращаемся к первой таблице from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) -- создаем и джойним вторую таблицу left join ( select * from ( values (&#39;2021-11-06&#39;, &#39;grp1&#39;) ) as tb2(var4, var3) ) as t2 using(var3) Table 11: 2 records var3 var1 var2 var4 grp1 1 a 2021-11-06 grp1 2 b 2021-11-06 Более простой пример с уже существующими таблицами: select * from chars left join ( select planet_name, gravity from planets where climate = &#39;temperate&#39; ) as p using(planet_name) Table 12: Displaying records 1 - 10 planet_name row.names name height mass hair_color skin_color eye_color birth_year gender url gravity Tatooine 1 Luke Skywalker 172 77 blond fair blue 19BBY male https://swapi.co/api/people/1/ NA Tatooine 2 C-3PO 167 75 n/a gold yellow 112BBY n/a https://swapi.co/api/people/2/ NA Tatooine 3 Darth Vader 202 136 none white yellow 41.9BBY male https://swapi.co/api/people/4/ NA Tatooine 4 Owen Lars 178 120 brown, grey light blue 52BBY male https://swapi.co/api/people/6/ NA Tatooine 5 Beru Whitesun lars 165 75 brown light blue 47BBY female https://swapi.co/api/people/7/ NA Tatooine 6 R5-D4 97 32 n/a white, red red unknown n/a https://swapi.co/api/people/8/ NA Tatooine 7 Biggs Darklighter 183 84 black light brown 24BBY male https://swapi.co/api/people/9/ NA Tatooine 8 Anakin Skywalker 188 84 blond fair blue 41.9BBY male https://swapi.co/api/people/11/ NA Tatooine 9 Shmi Skywalker 163 NA black fair brown 72BBY female https://swapi.co/api/people/43/ NA Tatooine 10 Cliegg Lars 183 NA brown fair blue 82BBY male https://swapi.co/api/people/62/ NA Также вложенные запросы могут быть в блоке where: select * from chars where planet_name in ( select planet_name from planets where climate = &#39;temperate&#39; ) Table 13: Displaying records 1 - 10 row.names name height mass hair_color skin_color eye_color birth_year gender url planet_name 11 Boba Fett 183 78.2 black fair brown 31.5BBY male https://swapi.co/api/people/22/ Kamino 12 Lama Su 229 88.0 none grey black unknown male https://swapi.co/api/people/72/ Kamino 13 Taun We 213 NA none grey black unknown female https://swapi.co/api/people/73/ Kamino 19 Leia Organa 150 49.0 brown light brown 19BBY female https://swapi.co/api/people/5/ Alderaan 20 Bail Prestor Organa 191 NA black tan brown 67BBY male https://swapi.co/api/people/68/ Alderaan 21 Raymus Antilles 188 79.0 brown light brown unknown male https://swapi.co/api/people/81/ Alderaan 22 Obi-Wan Kenobi 182 77.0 auburn, white fair blue-gray 57BBY male https://swapi.co/api/people/10/ Stewjon 24 Han Solo 180 80.0 brown fair brown 29BBY male https://swapi.co/api/people/14/ Corellia 25 Wedge Antilles 170 77.0 brown fair hazel 21BBY male https://swapi.co/api/people/18/ Corellia 27 Jabba Desilijic Tiure 175 NA n/a green-tan, brown orange 600BBY hermaphrodite https://swapi.co/api/people/16/ Nal Hutta Common tables expressions Общие таблицы или “выражения с with” – крайне полезный инструмент, так как позволяет создавать в запросе временные таблицы (которые живут только во время запроса и нигде не созраняются) и обращаться к этим таблицам во время запроса. Для экспериментов удобно совмещать создание таблиц из заданных значений с помощью values и операции с этими таблицами с помощью with: -- указываем, что таблицы из запросов ниже будут временными и общими для всего запроса with -- создаем первую таблицу tmp1 as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) ), -- создаем вторую таблицу tmp2 as ( select * from ( values (&#39;2021-11-06&#39;, &#39;grp1&#39;) ) as tb2(var4, var3) ) --основная часть - пишем запрос к созданным таблицам select * from tmp1 left join tmp2 using(var3) Table 14: 2 records var3 var1 var2 var4 grp1 1 a 2021-11-06 grp1 2 b 2021-11-06 select experiments functions В блоке select можно использовать разные, временами сложные конструкции. Самое простое - какая-то операция с колонкой, например, вычисление среднего (для среднего в sql-диалектах используется функция avg()) или максимума. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(v1, v2, v3) ) select count(*) as n_rows, count(distinct v3) as n_groups, avg(v1) as v2_avg from tmp Table 15: 1 records n_rows n_groups v2_avg 2 1 1.5 case Немного более сложный, но очень полезный инструмент - оператор логического ветвления. В R это аналог switch или вложенных ifelse. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- открываем логическое ветвление case -- первое условие when v1 &lt; 3 then &#39;g1&#39; -- второе условие when v1 = 3 then &#39;g2&#39; -- третье условие - &quot;все прочее&quot; else &#39;g3&#39; -- закрываем ветвление и указываем, как назвать колонку end as grp2 from tmp Table 16: 5 records v1 v2 v3 grp2 1 a grp1 g1 2 b grp1 g1 3 NA grp1 g2 4 d grp2 g3 5 e grp2 g3 filter Полезная, но достаточно малоизвестная конструкция - значения в колонках можно фильтровать по значениям других колонок. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select -- считаем количество строк, в которых в v3 есть значение grp1 count(*) filter(where v3 = &#39;grp1&#39;), -- одновременно считаем количество значений в колонке v1, для которых в v3 есть значение grp2 count(v1) filter(where v3 = &#39;grp2&#39;) from tmp Table 17: 1 records count count 3 2 Window functions row_number() over () Select-запросы в SQL предназначены в первую очередь для извлечения подвыборок (из одной или нескольких таблиц, с определенным составом колонок). Поэтому какие-то более сложные операции бывает достаточно сложно сделать. Одними из таких операций являются действия с колонками, в которых учитываются значения колонки в предыдущих строках - например, кумулятивная сумма или сумма в определенном окне (количестве строк до текущей) и тому подобные. Такие операции делаются в SQL с помощью оконных функций, где под окном понимается определенный набор строк колонки, с которыми надо выполнить какие-то операции. Один из самых простых видов оконных функций - нумерация строк: with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- row_number() - функция определения номера, over() - определение окна. -- так как в over() ничего не указано, под окном понимаются все строки таблицы row_number() over() as counter from tmp Table 18: 5 records v1 v2 v3 counter 1 a grp1 1 2 b grp1 2 3 NA grp1 3 4 d grp2 4 5 e grp2 5 over (partition by) Оконные операции можно выполнять в группах по значениям какой-то колонки, так же при этом можно сортировать строки по другим колонкам: with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- указываем, что окно бьется на группы в зависимости от значений v3 row_number() over(partition by v3) as counter, -- указываем, что окно бьется на группы в зависимости от значений v3 -- и одновременно сортируем значения по убыванию в зависимости от колонки v1 row_number() over(partition by v3 order by v1 desc) as counter_rev from tmp order by v1 Table 19: 5 records v1 v2 v3 counter counter_rev 1 a grp1 1 3 2 b grp1 2 2 3 NA grp1 3 1 4 d grp2 1 2 5 e grp2 2 1 total sum Другой пример запроса с оконной функцией – считаем общую сумму по колонке по всей таблице и записываем ее в отдельную колонку (значение суммы одно, просто размножается по количеству строк). with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- считаем сумму v1 по всем строкам таблицы sum(v1) over() as total_sum from tmp Table 20: 5 records v1 v2 v3 total_sum 1 a grp1 15 2 b grp1 15 3 NA grp1 15 4 d grp2 15 5 e grp2 15 cumulative sum Более сложная конструкция для вычисления кумулятивной суммы. Здесь мы указываем, что хотим посчитать не просто сумму, а кумулятивную сумму. Кумулятивная сумма представляется как сумма всех значений колонки от начала и до текущей строки – окно, в котором считается сумма, с каждой строкой расширяется. Такое поведение задается аргументом range, в котором указывем границы (можно и другие границы указать): with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- для каждой строки считаем сумму v1 от начала до текущей строки sum(v1) over(order by v1 range between unbounded preceding and current row) as cum_sum from tmp Table 21: 5 records v1 v2 v3 cum_sum 1 a grp1 1 2 b grp1 3 3 NA grp1 6 4 d grp2 10 5 e grp2 15 explain plan Для оптимизации можно посмотреть план запроса, который составляет оптимизатор. Умение читать и интерпретировать подобные планы приходит с опытом, чем больше - тем лучше, я не настолько хорошо знаю эту область, чтобы полноценно про нее рассказывать. Здесь просто для иллюстрации, что такое вообще есть. explain select * from chars where planet_name = &#39;Naboo&#39; Table 22: 2 records QUERY PLAN Seq Scan on chars (cost=0.00..2.96 rows=11 width=94) Filter: (planet_name = ‘Naboo’::text) analyze Когда мы явно указываем analyze, оптимизатор не просто создает план запроса, а реально выполняет запрос и выводит, сколько времени потребовалось выполнение того или иного этапа запроса. explain analyze select * from chars where planet_name = &#39;Naboo&#39; Table 23: 5 records QUERY PLAN Seq Scan on chars (cost=0.00..2.96 rows=11 width=94) (actual time=0.023..0.024 rows=11 loops=1) Filter: (planet_name = ‘Naboo’::text) Rows Removed by Filter: 66 Planning time: 0.059 ms Execution time: 0.046 ms "],["ab-тесты.html", "A/b-тесты Запись занятия Принятие решений Проверка гипотез Что такое А/В-тесты A/B-тесты в жизни продукта Дизайн А/В-тестов Ошибки в А/В-тестах Pro et contra Полезные материалы", " A/b-тесты Запись занятия Принятие решений Экспертная оценка Data-driven подход Data-informed подход Проверка гипотез Гипотеза - предположение о причинах наблюдаемого поведения Отличия от академических гипотез: меньше требования к точности больше опоры на согласованность с картиной мира разные инструменты (не только статистика) Что такое А/В-тесты Определение Аб-тесты пришли из экспериментальных наук, в частности, из медицины, психологии, даже сельского хозяйства. Основная идея - сравниваем разные варианты - разные методы лечения, разные условия развития, в общем виде - разные методы воздействия. А/В-тесты (сплит-тесты) - способ понять, станет ли продукт лучше, изменив часть продукта и сравнив с неизмененной частью А - контрольная группа, группа без изменений В - тестовая группа Области применения E-commerce / маркетинг - тестирование лендингов, иконок в сторе, рекламных креативов UI - дизайн пользовательских интерфейсов, например, цвет или форма кнопок Продукт - новые фичи, будь то предложения, схемы монетизации, новый контент или же просто функционал Оффлайновые тесты - например, расстановка продуктов в магазинах A/B-тесты в жизни продукта Cтартапам тесты не нужны Тесты - достаточно сложная парадигма, а стартапы в первую очередь ориентированы на реализацию идеи продукта, ключевые метрики. К тому же слишком много изменений - и на новых продуктах, и в каждом релизе у нас с десяток фичей, непонятно, как это тестировать. Или очень дорого. Да и в прод проще выкатить, так как на малом количестве пользователей можно позволить себе больше экспериментов При этом можно тестировать самые базовые вещи на этапе концепта, в играх это сеттинг и визуальный стиль. Буквально парой фейковых страниц в сторе. А/В-тесты нужны для улучшения продукта Аналитика больше для зрелых компаний, которые задумываются о сокращении расходов, а также приходят к стадии улучшения качества фичей и дополнительного функционала. Полишинг и экономия. А/В-тесты и другие инструменты Дизайн А/В-тестов Общая структура сформулировать и приоритезировать гипотезы выбрать метрику определить размер выборки разбить на группы показать пользователям проанализировать рассказать о результатах Cоставление и выбор гипотезы Какие гипотезы стоит проверять Высокая ценность и высокий риск. Тестируйте, тщательно исследуйте все детали. Высокая ценность, уверенность и низкие риски. Запускайте в первую очередь. Низкая ценность и низкие риски. Не тестируйте и, возможно, не запускайте вовсе. Низкая ценность и высокие риски. Не стоит ваших усилий, в помойку. выбор метрики Почему мы уверены, что эта фича окажет влияние? Нередко хотят тестировать какие-то минорные изменения, например, чуть скругленная иконка или чуть более яркие плашки в банке. Для компаний типа Гугла это нормально, но для нас бессмысленно - у нас есть более сильные факторы и гипотезы. Общая идея - тестировать надо то, что хотя бы в теории может сильно повлиять на продукт. И проведение теста сильно меньше возможной прибыли. Новое предложение, новая схема монетизации и так далее. Если фича не может повлиять на продукт - то зачем она нужна? Также, если фича даже в теории не окажет негативного эффекта, то можно и так пушить. Впрочем, для компаний, которые борются за доли процента в метриках, это может быть иначе - там затраты меньше, чем возможная прибыль. Как мы поймем, что фича оказала влияние? Очень большая проблема - нередко фичи делаются просто потому что делаются. Так было запланировано, так захотел продакт, начальник или кто-то еще. Важно понимать, как мы будем измерять эффект. Потому что если нет измерения - то смысл делать аб-тест? К тому же, понимание, на что влияет тестируемый фактор - это всегда понимание модели процесса, что должно измениться в первую очередь. Второе - понимание, какой будет эффект, нужно для расчеты выборки. Для слабых эффектов нужна большая выборка, а большая выборка может быть долго и дорого. разбиение на группы А/В, A/B/n-тесты Классический дизайн - есть контрольная группа, есть тестовая. Иногда тестовых групп может быть несколько. В редких случаях нет контрольной - когда мы выбираем что-то из альтернативы (лечение, схему монетизации и т.д.) АА/BB-тесты Специфичный вид. На мой взгляд, несколько устарелый - когда есть основания предполагать, что тестовая и контрольная группы неоднородны, их сплитят и сравнивают. ухудшающие А/В-тесты Нетривиальный дизайн, когда тестовая группа - это ухудшение продукта. Главное, создать разницу между группами. отрицательные А/В-тесты Редкий зверь. Вариация, когда выключается фича полностью (например, низкочастотная) и смотрим, как пользователи реагируют. На группы разбивать можно по-разному - случайным образом 50/50, в некоторых случаях меньшие доли - 80/20, у меня был кейс 85/10/5, потому что на систему монетизации. размер аудитории Какому количеству пользователей показывать? Всегда хочется сократить количество пользователей - их либо надо закупать (а это может быть дорого, например, 3-7 долларов), либо если они будут хуже платить, то страшно потерять деньги. Но при этом пользователей должно быть достаточно, чтобы проводить стат.анализ результатов. Все методы имеют ограничения, и в данном случае надо помнить, что чем слабее ожидаемый эффект, тем больше нужна выборка. Как долго вести тест? Это больше зависит от ситуации в продукте и ожиданиях топов. Вряд ли кто-то готов будет ждать несколько месяцев, пока наберется группа. С другой стороны, есть технические ограничения, например, нельзя за один день закупить пользователей. Плюс есть всякие факторы сезонности, праздники и проч. Это уже в ошибках тестов. размер выборки ожидаемый эффект доверительные интервалы Ошибки в А/В-тестах Ошибка парадигмы Вообще не проводить АВ-тесты Выкатить фичу на прод может быть в разы дороже, чем сделать аб-тест. Выкатить много фичей на прод одновременно - тем более, непонятно, что будет влиять. А аб-тесты структурируют план экспериментов Ошибки реализации зафиксированные тестовые и контрольная группы В настоящее время редкая, но все же встречающаяся ошибка - когда пользователь навсегда попадает в какую-то группу. Вызвано, как правило, тем, что разработчики что-то не учли, а аналитики не проконтролировали. Вредно тем, что в какой-то момент пользователи тестовой и контрольной групп начинают сильно отличаться по опыту и, соответственно, могут по-разному реагировать на тесты. технические проблемы при разделении групп Как правило, когда пользователи попадают сразу в две группы, или метка не соответствует реальности. Такое может быть, особенно когда меняется система идентификации пользователя (истекают куки сайта). Есть и целенаправленные ситуации, когда производится подтасовка - кейс RetailRocket и Rees, в котором уводились пользователи из сегмента. Ошибка в реализации тестируемых моделей Как-то мы тестировали простую схему - одним пользователям давали фиксированную скидку, другим - в зависимости от их платежной истории. Соответственно, сегменты вычислялись достаточно сложно. Важно было правильно определять сегмент и скидку для пользователя, перепроверяли работу серверников в холостых тестах. Общий вывод - аналитики должны контролировать то, как технически реализованы тесты. Ошибки дизайна игнорирование специфики метрики Надо учитывать, как ведет себя метрика, на изменение которой мы ориентируемся в тесте. Специфика метрики может накладывать свои результаты. Например, может быть недельное колебание удержания, и тогда тест надо проводить неделю, так как при коротких тестах на пару дней можно получить завышенные или заниженные результаты. Аналогично с активностью. Могут быть более длинные эффекты - например, начало учебного года или новогодние праздники. тестировать надо на той же выборке, на которой будем применять Тестировать платежку на жителях Филиппин можно, но странно. Но делают. Аналогично - тестировать на доп.офисах в спальных районах и в центрах города (или в провинции и в москве) - странная идея. должны быть исключены внешние факторы (вести группы параллельно) Последовательные группы (две рекламные кампании) или две версии - это не аб-тест. Потому что есть куча побочных факторов, которые сложно проконтролировать. тестируется сразу несколько изменений Частая ошибка, когда на одной и той же выборке тестируется два или больше вариантов. Проблема в том, что в результате получается не 2 группы, а 4, а выборка уменьшается. предложения в тестовой и контрольной группе неоднородны Иногда бывает так, что само предложение в тестовой группе влияет на целевую метрику. Например, когда мы предлагаем два пакета за одну цену, но в одном скидка чуть больше. перебор вариантов руками пользователей (рост ошибки I рода) Кейса гугла. 41 вариант цвета ссылки. Общая логика проверки гипотез - у нас есть до 5% вероятности, что группы все-таки не различаются, случайно нашли то, чего нет. Вообще, это именно ошибка дизайна - группы должны формироваться из какой-то конкретной гипотезы, а не просто искать хороший вариант. Ошибки в анализе подсматривание результатов в процессе Всем хочется побыстрее узнать результаты, поэтому смотрят постоянно на группы. Это ошибка, потому что всегда есть вариативность и даже неразличимые группы в некоторые моменты могут различаться. Притом, чем чаще подсматриваешь, тем выше вероятность сделать ложный вывод. Притом, подсматривать можно, но выводы делать только после набора выборки. Варианты решения - использование неклассических стат.парадигм, в частности, последовательного семплинга. игнорируется форма распределения Чисто техническая проблема, когда применяются некорректные методы проверки стат.гипотез игнорируется размер эффекта Важно вообще понимать, какой рычаг у теста. Ошибки в интерпретации не учитываются долгосрочные эффекты Часто вывод по аб-тестам делается в достаточно быстро после окончания теста. При этом игнорируются какие-то долгосрочные эффекты. Например, если пользователь сейчас купил товар по высокой скидке (в тестовой группе больше продаж и прибыль), надо посмотреть, а как он дальше будет покупать. В играх это проблема с банком и ростом прогрессии - в начале игры маленьких платежей достаточно для победы, на дальних этапах уже нет. При тестировании на Филиппинах можно сильно ошибиться. не учитываются косвенные эффекты (как в целом реагирует система) Может быть каннибализация - когда эффект от тестовой фичи съедает эффекты от других фич. Пользователь купил по скидке, и не стал покупать другие предложения. Тестовая группа выиграла, но в тестовой группе совокупно просели платежи. Может быть и обратная сторона - в тестовой группе платежи просесть могли по другой причине, не в тестовой фиче. слабые или отсутствующие эффекты интрепретируются как тенденции Зона “когда очень хочется” - статистически есть достаточно строгие правила принятия решений, при этом даже при отсутствующих различиях хочется интерпертировать некоторые показатели. Иногда просто показаны средние, но не показаны дисперсии (а если выборка маленькая, то штормить может сильно). отсутствие положительных различий воспринимается как неудача Все как в науке, если гипотеза не подтвердилась, то ты неудачник. Хотя аб-тесты, да и наука, в общем-то, не так работают. Отсутствие различий - тоже результат. Но да, он может быть болезнен для продакта. Иногда высказываются предположения, почему тест не сработал. Важно понимать, что это тоже гипотезы, и их тоже тестировать надо. И если бы эта гипотеза была правдоподобной, мы бы ее тестировали в первом тесте, додумались бы заранее. уверенность, что тест сломан, недостаточно долго идет и т.д. Когда тест не подтверждает гипотезу, в которую веришь - грустно и хочется продлить тест, как-нибудь отфильтровать выборку и так далее. Мой кейс с сегментированием пользователей по платежной истории - сложная система оказалась сравнима с нарисованной на коленке. неудачные эксперименты скрываются или подделываются Все как в науке - перекос в сторону открытий и новизны. В критических случаях результаты переинтрпретируются или даже подделываются. Pro et contra contra надо что-то менять Внедрение аб-тестов требует достаточно много времени и сил на разработку. Во-первых, сбор данных и разделения выборки на группы (кейс OK и Димы Бугайченко). Во-вторых, сами группы (системы ранжирования, выдачи, рекомендаций). требуют квалифицированных сотрудников АБ-тесты активно используют аппарат математической статистики, поэтому нужны люди, которые это умеют делать. В идеале, чтобы еще был опыт взаимодействия с продуктом, доменной областью. Этих людей надо искать или растить. не подтверждают гипотезы Далеко не все гипотезы подтверждаются, и это вызывает боль, спекуляции и постоянные сомнения - а вдруг что-то неправильно сделали в тесте, а гипотеза верная. pro мировой тренд на data-driven подход Общий тренд в мире в принятии решений - рациональное обоснование и попытка избежать когнитивных искажений, вызванных субъективным опытом. В этом смысле тут лидирует проверка гипотез на данных. Не чистый data-driven, но каждое решение проверяется на данных или согласовывается с данными. снижает вероятность неправильных решений и экономит деньги Разработка новой фичи может быть дорогая, эффект фичи может быть негативный и вести к убыткам. Но всегда будет сопротивление “это не фича плохая”, это сезон/праздники/трафик etc - то, что нельзя опровергнуть. Во-вторых, в тесте могут вскрыться доп.эффекты. Кейс Гарфилда из ГИ. уменьшает страх ошибки у рядовых сотрудников Когда есть система с понятной логикой принятия решения, человек, который выкатывает фичу, вынужден думать о том, что делать, если фича не работает. В идеале автоматическое тестирование и тестирование множества разных гипотез (аджайл-парадигма). автоматизация проверки многих гипотез В какой-то момент компании, которые динамично развиваются и широко работают с данными, приходят к идее систем автоматических тестов - в ОК такая система интегрирована в джиру, и тот, кто задает эксперимент, просто указывает когорты и группу метрик, а результаты сыпятся в комментарии. У нас тоже есть система аб-тестов, не настолько сильная, но и делаем мы ее недолго. Полезные материалы Очень хороший канал про A/B-тесты в телеграме дайджест всяких материалов, собирает Юрий Ветров (долгое время главный по дизайну в mail.ru) отличный материал от Олега Якубенкова, один из лучших по этой теме подглядываний в аб-тестах. Плюс есть набор ссылок. А/Б тестирование: от А до Б (неплохой набор ссылок) Ваши A/B-тесты сломаны (подробный разбор некоторых ошибок в тестах, хотя местами очень уж самовлюбленное выступление) Пара слов про выборку и размер эффекта. Немного спорное мнение про интерпретацию аб-тестов. Неплохой доклад Вита Черемисинова на конфе игроделов. Отличный плейлист по А/В-тестам от весьма известных в сообществе ребят. "],["stats.html", "Основы статистического вывода Запись занятия Основы стат.вывода Выборка Bootstrap Перестановочные тесты в R Критерий согласия \\(\\chi^2\\)-Пирсона Размеры эффекта Ошибка II рода Оценка выборки power.prop.test() Полезные ссылки Домашнее задание", " Основы статистического вывода Запись занятия Основы стат.вывода Выборка Выборка - часть генеральной совокупности Выборочные статистики нельзя напрямую интерпретировать как характеристики генеральной совокупности. Метрики центральных тенденций среднее: mean() медиана: median() мода (table() и другие методы оценки частот) Например, выберем случайным образом 20 чисел из ряда от 1 до 100, и посчитаем метрики центральных тенденций ряда. Надо помнить, что среднее менее устойчиво к отклонениям от нормального распределения, поэтому сравнение среднего и медианы может быть полезно для понимания данных, а медиана в целом более устойчивая к выбросам оценка. Метрики разброса размах: range() дисперсия: var() стандартное отклонение (\\(\\sigma\\)): sd() range(x) ## [1] 4 93 var(x) ## [1] 1031.671 sd(x) ## [1] 32.11964 Обычно используют размах и стандартное отклонение. Стандартное отклонение понятнее дисперсии, потому что в тех же единицах, что и измерение, поэтому можно сказать \\(\\overline{x} \\pm \\sigma\\). В целом, стандартное отклонение служит метрикой “точности измерения” - чем выше дисперсия, тем менее точное измерение. Для большинства процессов характерно увеличение точности (снижение дисперсии) измерения с увеличением выборки - чем больше выборка, тем меньше дисперсия. Однако это правило неприменимо, в частности, к равномерному распределению и некоторым другим ситуациям (процессам с бесконечной дисперсией). Нормальное распределение Нормальным называется распределение вероятностей, которое для одномерного случая задаётся функцией Гаусса. Нормальное распределение играет важнейшую роль во многих областях знаний. Случайная величина подчиняется нормальному закону распределения, когда она подвержена влиянию большого числа случайных факторов, что является типичной ситуацией в анализе данных. Поэтому нормальное распределение служит хорошей моделью для многих реальных процессов. Основные функции для работы с распределениями: d*() — функция вероятности (probability mass function) для дискретных распределений и функция плотности вероятности для непрерывных распределений. В практике аналитиков используется редко, нужна для понимания и визуализации теоретической формы распределения при разных параметрах. p*() — функция накопленной плотности распределения (cumulative distribution function; cdf), позволяет получить накопленную вероятность получить такое значение при условии, что оно принадлежит указанному распределению. Другими словами, с помощью этой функции можно получить вероятность получить такое значение или меньшее. Например, мы знаем, что в популяции средний мужчин распределен приблизительно нормально, со средним ростом \\(\\overline{M}\\) = 170 и \\(\\sigma\\) = 10. Попробуем понять, какова будет вероятность встретить человека с ростом 180 см или выше: pnorm(q = 180, mean = 170, sd = 10) ## [1] 0.8413447 Мы получаем значение 0.8413 - так как pnorm() использует функцию накопленной плотности, то это вероятность получить значение меньше 180 (то есть, 84% встреченных людей будет ниже 180 см.). Для того, чтобы оценить вероятность встретить значения больше, чем указанное (в нашем примере — встретить человека выше 180 см.) необходимо либо вычесть из единицы, либо воспользоваться аргументом lower.tail: 1 - pnorm(q = 180, mean = 170, sd = 10) ## [1] 0.1586553 pnorm(q = 180, mean = 170, sd = 10, lower.tail = FALSE) ## [1] 0.1586553 q*() — квантильная функция (quantile function) или обратная функция накопленной плотности распределения (inverse cumulative distribution function), позволяет получить значение исходя из квантиля накопленной плотности распределения. Таким образом, если следовать примеру с ростом людей в популяции, мы можем оценить, какова будет вероятность встретить людей из, допустим, верхнего квартиля (с какого роста можно говорить, что этот человек входит 25% самых высоких людей популяции): qnorm(p = 0.75, mean = 170, sd = 10) ## [1] 176.7449 Таким образом, если встреченный человек выше 176.74 см., то он выше чем 3/4 всех людей популяции. r*() — используется для генерации семплов из распределения определенной формы и с заданными параметрами. Например, генерируем семпл из нормального распределения: # генерируем данные сразу из стандартного нормального распределения x &lt;- rnorm(1000, mean = 0, sd = 1) x[1:10] ## [1] 0.9594941 -0.1102855 -0.5110095 -0.9111954 -0.8371717 2.4158352 ## [7] 0.1340882 -0.4906859 -0.4405479 0.4595894 C помощью rnorm() делаются выборки из распределения с заданными параметрами, однако чем меньше выборка, тем сильнее эмпирические значения среднего и стандартного отклонения будут отличаться от заданных при генерации: # мы задвали 0 mean(x) ## [1] -0.02189492 # мы задавали 1 sd(x) ## [1] 0.9992201 Префиксы p, r, d, q используются не только для нормального распределения (-norm()), в частности, для t-распределения, логнормального (-lnorm), равномерного (-unif) и т. д. Z-преобразование Преобразование (его иногда называют нормализацией), которое приводит нормально распределенные данные с произвольным средним и стандартным отклонением к стандартному нормальному распределению с \\(\\overline{M}\\) = 0 и \\(\\sigma\\) = 1: \\[z = \\frac{X_i-\\overline{M}}{\\sigma}\\] x &lt;- 1:10 scale(x) ## [,1] ## [1,] -1.4863011 ## [2,] -1.1560120 ## [3,] -0.8257228 ## [4,] -0.4954337 ## [5,] -0.1651446 ## [6,] 0.1651446 ## [7,] 0.4954337 ## [8,] 0.8257228 ## [9,] 1.1560120 ## [10,] 1.4863011 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.02765 Нормализация данных нужна в ситуациях, когда надо сравнивать выборки и распределений с разными параметрами или когда надо все измерения привести к одному масштабу. Допустим, у нас есть две выборки. Сравнивать распределение данных между выборками достаточно сложно, поэтому обычно сначала нормализуют, а потом сравнивают. Точно также нормализовать данные необходимо в некоторых алгоритмах типа кластерного анализа, чтобы масштаб измерения не вносил дополнительные эффекты. Проверка гипотез. NHST В классической статистике проверяется гипотеза, что группы не различаются (что разница средних равна нулю с высокой вероятностью, как в t-тесте). Концепция p-value исходит из этих условий — оценивается, насколько вероятно получить такое значение или большие различия. И если вероятность невысока (обычно за порог берется 5%, 0.05), то делается вывод, что вероятность нулевой разницы групп низкая и можно считать, что группы различаются. \\(H_0: \\overline{M_1} = \\overline{M_2}\\) Если формально, то p-value:— вероятность получить такие или более экстремальные значения при условии, что \\(H_0\\) истинна. Тестовые статистики и распределения Все критерии сравнения групп — это поиск какой-то метрики, по которой можно оценить степень различия. Это может быть как разница средних, так и соотношение групповой и межгрупповой дисперсий, и степень отклонения наблюдаемой вероятности от ожидаемой, и прочие метрики. Притом эти значения различия могут быть описаны разными (каждое своим) теоретическими распределениями: t, t-test, t.test(): сравнение средних значений групп F, ANOVA: сравнение внутригрупповой и межгрупповой дисперсий \\(\\chi^2\\), ci-square test, prop.test(): сравнение вероятностей в дискретных (биномиальном) распределениях В критериях вычисляется эта разница и нормируется на совокупную дисперсию групп (своеобразное z-преобразование), и потом вычисляется вероятность получить такую разницу (p-value). Допустим, у нас есть две группы: мужчины (\\(\\overline{M}\\) = 175, \\(\\sigma\\) = 10) и женщины (\\(\\overline{M}\\) = 165, \\(\\sigma\\) = 10): x_m &lt;- rnorm(25, mean = 175, sd = 10) x_f &lt;- rnorm(25, mean = 165, sd = 10) Мы хотим понять, значимо ли это различие между двумя группами или нет. Для этого мы формулируем нулевую гипотезу, которую и проверяем статистическими критериями: нулевая гипотеза всегда о том, что различий между группами нет. Мы никогда не можем принять гипотезу (потому что мы имеем дело с выборками, мало ли что попадется в других выборках), мы можем только найти существенные аргументы, чтобы ее отвергнуть. Таким аргументом будет оценка вероятность получить такое различие между группами, какое мы наблюдаем на наших выборках, при предположении, что различий нет (равны нулю). То есть мы обращаемся к теоретическому распределению меры различий, в котором математическое ожидание (среднее для нормального распределения) равно нулю и на этом распределении оцениваем, какова вероятность получить то значение, которое мы получили на наших выборках. Один из самый простых тестов для сравнения групп — критерий Стьюдента, он же t-тест. В этом тесте высчитывается тестовая статистика t — взвешенная на несмещенную оценку дисперсии разница средних двух групп (в нашем случае предполагается равенство дисперсий). Критерий t имеет свое собственное распределение, которое при больших значениях выборок приближается к нормальному. Если посмотреть на тело функции t.test(), то там видно, как используется t-распределение (tstat и df — вычисляемое значение t и степень свободы): pval &lt;- 2 * pt(-abs(tstat), df) alpha &lt;- 1 - conf.level cint &lt;- qt(1 - alpha/2, df) cint &lt;- tstat + c(-cint, cint) Соответственно, для проверки нулевой гипотезы с помощью t-критерия мы должны высчитать эмпирическое значение критерия t на наших данных и оценить, насколько вероятно получить такое или еще большее значение при условии, что наиболее вероятное значение (математическое ожидание) равно нулю. t.test(x_m, x_f, var.equal = TRUE, alternative = &quot;two.sided&quot;) ## ## Two Sample t-test ## ## data: x_m and x_f ## t = 2.0425, df = 48, p-value = 0.04661 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.08088702 10.27932238 ## sample estimates: ## mean of x mean of y ## 171.1373 165.9572 (alternative = “two.sided”, так предполагаем, что среднее группы x_f может быть больше среднего группы x_m, так и наоборот, x_f может быть больше группы x_m — то есть, мы должны учесть, что наше эмпирическое значение разницы между группами может быть как больше нуля, так и меньше нуля, в обоих хвостах распределения). В выводе функции мы видим значение t и оценку p-value. Перепроверим самостоятельно (df считается как n1 + n2 - 2, где n1 и n2 — численность первой и второй выборок, а 2 — количество выборок): pt(q = 3.8463, df = 25 + 25 - 2, lower.tail = FALSE) * 2 ## [1] 0.000353164 (на 2 умножаем, потому что смотрим оба хвоста распределения). Bootstrap Техника “размножения выборки”. Если считать, что собранная выборка репрезентативна относительно ген.совокупности, то ресемплы из этой выборки так же будут в какой-то мере репрезентативны. Соответственно, если сделать множество ресемплов (в том числе и с возвращениями) и в каждом ресемпле считать какую-нибудь статистику, например, среднее - то можно получить распределение среднего значения. И это распределение будет вполне отражать ген.совокупность. Соответственно, можно таким образом численно определить границы доверительного интервала среднего. Техника хороша тем, что устойчива к форме распределения данных, и не опирается на них. Во-вторых, очень полезна для работы с малыми выборками и мета-анализами. Техники бутстрепа разные, от выкидывания по очереди элемента (jacknife) до перевыборки или перемешивания (используется для сравнения групп): jacknife resample permutation Перестановочные тесты Логическое продолжение парадигмы бутстрепа - если ресемплить не какую-то стат.метрику (типа t или F), а разницу между группами. При этом проверяется не просто разница между двумя семплами, а насколько вероятно вообще такое разбиение - грубо говоря, множество раз группы перемешиваются и переразбиваются, и в каждом новом разбиении считается различие (различие средних, нередко просто больше или меньше). В результате получается выборка значений различия, и потом оценивается, насколько вероятно было получить то разбиение, которое было исходно в эксперименте. Таким образом вместо принятия решения о нулевой гипотезе на основе теоретических распределений, решение принимается на основе эмпирического распределения - это позволяет сравнивать группы, наблюдения в которых распределены отличным от нормального образом. Перестановочные тесты в R При желании можно написать свою реализацию но чаще используют пакет coin, в частности функцию oneway_test(). library(coin) ## Loading required package: survival # собираем табличку, чтобы была группирующая переменная x_dt &lt;- data.table( group = factor(rep(c(&#39;male&#39;, &#39;female&#39;), each = 25)), height = c(x_m, x_f) ) # Fisher-Pitman permutation test oneway_test(height ~ group, data = x_dt, distribution = approximate(nresample = 10000)) ## ## Approximative Two-Sample Fisher-Pitman Permutation Test ## ## data: height by group (female, male) ## Z = -1.9795, p-value = 0.0448 ## alternative hypothesis: true mu is not equal to 0 Критерий согласия \\(\\chi^2\\)-Пирсона Продуктовые аналитики чаще всего имеют дело с биномиальными данными - уровнем конверсии, уровнем удержания и т.д. То есть с теми процессами, где есть какая-то доля успешных и неуспешных исходов. Для проверки гипотез о различии групп по доле успешных событий используется обычно \\(\\chi^2-критерий Пирсона\\), он же критерий согласия. Основная идея - оценить разницу в вероятности полученной доли успешных событий и ожидаемой доли, это делается для каждого из исходов. Нормированная сумма квадратов этих разниц и составляет значение \\(\\chi^2\\), по теоретическому распределению которого и определяется p-value: \\[\\chi^2_n = \\sum_{i=1}^n{\\frac{(O_i - E_i)^2}{E_i}}\\] Классический пример - есть монетка, ее подбросили 60 раз. 20 раз выпал орел и 40 раз выпала решка. Насколько случайно такое соотноешние, может ли быть так, что одна из сторон монетки искусственно утяжелена? В R д я ответа на этот вопрос можно воспользоваться функцией prop.test() (в который по умолчанию включается поправка Йетса на непрерывность). Ожидаемые вероятности для монетки с равными сторонами будут 30 раз орел и 30 раз решка, поровну. Укажем вектор с количеством наблюдаемых исходов и вектор с количеством испытаний: prop.test(x = c(20, 30), n = c(60, 60)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(20, 30) out of c(60, 60) ## X-squared = 2.7771, df = 1, p-value = 0.09562 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.35721175 0.02387842 ## sample estimates: ## prop 1 prop 2 ## 0.3333333 0.5000000 Как мы видим, вероятность получить 20 (или еще меньше) выпадений орлов при 60 подбрасываниях составляет около 9.5%. Это не очень много, но все равно больше конвенционального порога в 5% (p-value = 0.05), поэтому мы можем сделать вывод, что получить 20 орлов при 60 случайных подбрасываниях маловероятно, но все же вполне возможно. То есть, соотношение 20/40, полученное в эксперименте незначимо отличается от случайного (30/30), и монетка сбалансирована корректно. Размеры эффекта В какой-то момент пришли к идее, что просто сравнивать группы на значимость различий недостаточно - хорошо бы понимать, как сильно различаются группы. Из этого выросло целое направление, power analysis, где в фокусе анализа размеры эффектов и их взаимосвязь с размером выборки, а так же ошибками первого и второго рода. Индексов размера эффекта много - и для непрерывных переменных, и для дискретных, и для оценки взаимосвязи. В том числе существуют индексы для сложных моделей, таких как дисперсионный анализ с взаимодействием факторов и т.д. Обычно из одного индекса некоторыми преобразованиями можно получить другой индекс. Ошибка II рода Когда мы говорим о нулевой гипотезе, мы предполагаем, что различий между группами нет. Соответственно, мы оцениваем теоретическую вероятность (p-value) получить такое или более экстремальное значение, как в нашем эксперименте. Для этого мы вычисляем разницу между группами и смотрим по кривой распределения этой метрики различия, какая вероятность получить такое значение или больше/меньше (в зависимости о того, левый или правый край распределения мы берем во внимание). При этом есть расширение этого подхода — когда мы отвергаем нулевую гипотезу (говорим, что маловероятно получить наше эмпирическое значение при предположении, что группы не различаются), мы утверждаем, что разница между группами принадлежит тому же по форме распределению, но с другими параметрами. На примере нормального распределения (и z-критерия) это будет означать, что наше эмпирическое значение разницы принадлежит не распределению с mean = 0, а распределению с ненулевым средним (т. е. смещенным по оси OX вправо). Таким образом мы приходим к тому, что мы можем оценить, какова вероятность получить такое значение, если группы не различаются (ошибка I рода, она же ошибка ложного срабатывания, когда мы утверждаем, что различие есть, когда его на самом деле нет). И мы можем оценить, какова вероятность, что это значение на самом деле принадлежит распределению параметра, описывающему ситуацию, когда группы действительно различаются. Эту вероятность называют мощностью теста (\\(\\beta\\)), \\(1 - \\beta\\) — ошибка II рода (вероятность пропуска, когда мы утверждаем, что различия нет, а на самом деле оно есть). Ошибки I и II рода хорошо иллюстрирует вот такая картинка: Оценка выборки Все четыре ключевых термина: размер выборки, размер эффекта, ошибка I рода и ошибка II рода (мощность теста) тесно взаимосвязаны между собой. По сути зная три из них, можно оценить четвертую. Так, размер выборки влияет на форму распределения метрики разницы групп (насколько длинные и тонкие хвосты); размер эффекта — на величину сдвига по оси OX распределения метрики, описывающей ситуацию, когда группы не различаются); ошибка I рода — какую, условно, часть хвоста распределения мы считаем маловероятные значения при нулевом различии групп; ошибка II рода — насколько сильно пересекаются наш хвост из распределения отсутствии различия и распределение при предположении, что группы правда различаются. Есть очень хорошее приложение, которое визуализирует эту связь (картинка кликабельна): power.prop.test() Для оценки мощности теста и для оценки выборки исходя из заданных условий (размеры ошибок I и II рода, ожидаемого размера эффекта) есть достаточно много инструментов и алгоритмов. Один из весьма популярных — G*Power. При этом стоит учитывать, что чем сложнее дизайн эксперимента (многофакторные влияния, внутри- и межгрупповые планы, наличие случайных эффектов и т. д.), тем сложнее для выведения и, соответственно, сомнительнее оценки мощности теста и расчеты выборки. В R для анализа мощности или оценки выборки для теста пропорций (именно этим тестом мы обычно проверяем значимость в различии конверсии или удержания) можно воспользоваться функцией power.prop.test(). Функция принимает на вход размер каждой группы (предполагается, что группы одинаковы), доля успешных событий для первой группы, доля успешных событий для второй группы, уровень значимости и мощность теста. В том случае, если в какой-то из этих аргументов выставить NULL, то в результатах теста будет предложено необходимое значение. То есть, зная наблюдаемую вероятность события в двух группах, а так же имея требования к точности эксперимента (ур.значимости и мощность, ошибки I и II рода) можно вычислить необходимую выборку. Или, например, имея выборку, вероятность события в одной группе и требования к уровням ошибок I и II рода, можно оценить какую значимую разницу можно почувствовать на этой выборке. Аргументы функции: args(power.prop.test) ## function (n = NULL, p1 = NULL, p2 = NULL, sig.level = 0.05, power = NULL, ## alternative = c(&quot;two.sided&quot;, &quot;one.sided&quot;), strict = FALSE, ## tol = .Machine$double.eps^0.25) ## NULL Простейший пример, у нас есть удержание пользователей на уровне 35%, мы хотим протестировать новый функционал, который, по нашим ожиданиям, повысит удержание на 2%. Сколько необходимо пользователей, с учетом стандартных требований к точности и мощности теста (ошибка первого рода - 5%, ошибка второго рода - 20%). Воспользуемся функцией power.prop.test()и прямо укажем, что нам неизвестна выборка: power.prop.test(n = NULL, p1 = 0.35, p2 = (0.35 + 0.02), sig.level = 0.05, power = (1 - 0.2)) ## ## Two-sample comparison of proportions power calculation ## ## n = 9040.73 ## p1 = 0.35 ## p2 = 0.37 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group # power.prop.test(n = NULL, p1 = 0.35, p2 = 0.37, sig.level = 0.05, power = 0.8) Как мы видим, нам необходимо не менее 9 тысяч пользователей в каждой группе аб-теста - в таком случае мы можем утверждать, что различия в удержании между группами в 2% и выше неслучайны. Если же мы попробуем задать количество пользователей каждой группы (например, у нас мало денег и мы не можем привлечь много пользователей), среди которых мы ожидаем получить значимое различие в 2% (при том же уровне ошибки первого рода в 5%), то у нас резко упадет мощность. Это значит, что велика будет вероятность на такой выборке пропустить эффект и сказать, что различия нет, хотя оно на самом деле есть: power.prop.test(n = 1000, p1 = 0.35, p2 = (0.35 + 0.02), sig.level = 0.05) ## ## Two-sample comparison of proportions power calculation ## ## n = 1000 ## p1 = 0.35 ## p2 = 0.37 ## sig.level = 0.05 ## power = 0.1518592 ## alternative = two.sided ## ## NOTE: n is number in *each* group Обычно оценку мощности и расчет выборки делают до эксперимента, так как после эксперимента оценивать ошибку II рода несколько бессмысленно, эксперимент-то уже проведен. Полезные ссылки Глава из учебника по R и статистике моего коллеги по психфаку Ивана Позднякова Онлайн-курс Толи Карпова по основам статистики. Один из самыйх известных русскоязычных онлайн-курсов. В целом довольно неплох, рекомендую обратить внимание на первые пару блоков (введение и сравнение средних). Расчет критерия \\(\\chi^2\\) вручную. Очень хорошо описана логика расчета. Еще один хороший пример расчета. Неплохо описана идея степеней свободы и формы распределения. Один из лучших учебников по размерам эффекта. По ссылке только название и ссылка на амазон, но при желании файл можно найти в сети. Домашнее задание level 1 (IATYTD) Прочитать статью и вручную на бумаге повторить расчеты критерия \\(\\chi^2\\) и интерпретацию значения критерия по таблице (ссылка на нее в самом низу страницы). level 2 (HNTR) Повторить расчет в R. level 3 (HMP) Прочитать статью и вручную на бумаге повторить расчеты t-критерия и интерпретацию значения критерия по таблице (ссылка на нее в самом низу страницы). level 4 (UV) Повторить расчет в R. Для этого вам потребуется сгенерировать два семпла распределения с указанными параметрами. level 5 (N) Нарисовать кривые \\(\\chi^2\\) и t-распределений, которые помогут принять решение о том, принимаем мы или нет нулевую гипотезу (вместо таблиц значений критерия). Для этого вам потребуются функции семейства d*(). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
